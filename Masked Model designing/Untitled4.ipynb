{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e211692-30f2-4765-80c6-6b2ea247fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback word: the\n",
      "\n",
      "--- Debug: Sample Test Contexts ---\n",
      "Sentence 1: The sweat stood upon it in <MASKED> .\n",
      "Tokens: ['the', 'sweat', 'stood', 'upon', 'it', 'in', 'masked']\n",
      "Left Context: ('it', 'in')\n",
      "Right Context: ()\n",
      "---\n",
      "Sentence 2: The city was named for Judge <MASKED> R McKee .\n",
      "Tokens: ['the', 'city', 'was', 'named', 'for', 'judge', 'masked', 'r', 'mckee']\n",
      "Left Context: ('for', 'judge')\n",
      "Right Context: ('r', 'mckee')\n",
      "---\n",
      "Sentence 3: A <MASKED> of girls are cheering .\n",
      "Tokens: ['a', 'masked', 'of', 'girls', 'are', 'cheering']\n",
      "Left Context: ('a',)\n",
      "Right Context: ('of', 'girls')\n",
      "---\n",
      "Sentence 4: Tom resigned as he wasnt <MASKED> valued at work .\n",
      "Tokens: ['tom', 'resigned', 'as', 'he', 'wasnt', 'masked', 'valued', 'at', 'work']\n",
      "Left Context: ('he', 'wasnt')\n",
      "Right Context: ('valued', 'at')\n",
      "---\n",
      "Sentence 5: In the disastrous days that followed Maurice was subject to Fredericks <MASKED> .\n",
      "Tokens: ['in', 'the', 'disastrous', 'days', 'that', 'followed', 'maurice', 'was', 'subject', 'to', 'fredericks', 'masked']\n",
      "Left Context: ('to', 'fredericks')\n",
      "Right Context: ()\n",
      "---\n",
      "\n",
      "--- Debug: Sample Predictions ---\n",
      "Test Sentence: The sweat stood upon it in <MASKED> .\n",
      "Predicted Word: time\n",
      "---\n",
      "Test Sentence: The city was named for Judge <MASKED> R McKee .\n",
      "Predicted Word: the\n",
      "---\n",
      "Test Sentence: A <MASKED> of girls are cheering .\n",
      "Predicted Word: group\n",
      "---\n",
      "Test Sentence: Tom resigned as he wasnt <MASKED> valued at work .\n",
      "Predicted Word: was\n",
      "---\n",
      "Test Sentence: In the disastrous days that followed Maurice was subject to Fredericks <MASKED> .\n",
      "Predicted Word: the\n",
      "---\n",
      "\n",
      "Submission file created: sample_submission2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Remove punctuation, convert to lowercase, and split the sentence into tokens.\n",
    "    Note: This removes characters like '<' and '>', so \"<MASKED>\" becomes \"masked\".\n",
    "    \"\"\"\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    tokens = sentence.split()\n",
    "    return tokens\n",
    "\n",
    "def extract_contexts(tokens, window=2):\n",
    "    \"\"\"\n",
    "    For each word in the tokenized sentence, extract its left and right contexts using a fixed window.\n",
    "    Returns a list of tuples: (left_context, word, right_context).\n",
    "    \"\"\"\n",
    "    contexts = []\n",
    "    for i in range(len(tokens)):\n",
    "        left_context = tokens[max(0, i - window):i]\n",
    "        right_context = tokens[i+1:min(len(tokens), i + window + 1)]\n",
    "        contexts.append((tuple(left_context), tokens[i], tuple(right_context)))\n",
    "    return contexts\n",
    "\n",
    "def build_context_dict(sentences, window=2):\n",
    "    \"\"\"\n",
    "    Build a dictionary from the training sentences.\n",
    "    Keys are tuples (left_context, right_context) and values are dictionaries mapping words to their frequency.\n",
    "    \"\"\"\n",
    "    context_dict = {}\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        for left_context, word, right_context in extract_contexts(tokens, window):\n",
    "            key = (left_context, right_context)\n",
    "            if key not in context_dict:\n",
    "                context_dict[key] = {}\n",
    "            context_dict[key][word] = context_dict[key].get(word, 0) + 1\n",
    "    return context_dict\n",
    "\n",
    "def build_left_index(context_dict):\n",
    "    \"\"\"\n",
    "    Pre-compute an index mapping each token that appears in any left context\n",
    "    to the set of context keys (tuple of left and right contexts) where it appears.\n",
    "    \"\"\"\n",
    "    left_index = defaultdict(set)\n",
    "    for key in context_dict.keys():\n",
    "        left_context, _ = key  # key = (left_context, right_context)\n",
    "        for token in left_context:\n",
    "            left_index[token].add(key)\n",
    "    return left_index\n",
    "\n",
    "def get_most_common_word(sentences):\n",
    "    \"\"\"\n",
    "    Compute the most common word in the provided sentences.\n",
    "    \"\"\"\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenize(sentence)\n",
    "        word_counts.update(tokens)\n",
    "    if word_counts:\n",
    "        return word_counts.most_common(1)[0][0]\n",
    "    return \"the\"\n",
    "\n",
    "def get_mask_context(tokens, mask_token=\"masked\", window=2):\n",
    "    \"\"\"\n",
    "    Given a tokenized sentence, find the mask token and return its left and right contexts.\n",
    "    Handles edge cases when the masked word is at the beginning or end.\n",
    "    \"\"\"\n",
    "    if mask_token not in tokens:\n",
    "        return None, None\n",
    "    mask_index = tokens.index(mask_token)\n",
    "    left_context = tuple(tokens[max(0, mask_index - window):mask_index])\n",
    "    right_context = tuple(tokens[mask_index+1:mask_index+window+1])\n",
    "    return left_context, right_context\n",
    "\n",
    "def predict_masked_word(sentence, context_dict, left_index, fallback_word, mask_token=\"masked\", window=2):\n",
    "    \"\"\"\n",
    "    Predict the masked word in a sentence.\n",
    "    1. Try an exact match on the context.\n",
    "    2. If none, use a relaxed matching strategy by only considering context keys \n",
    "       that appear in the left_index for any token in the test left context.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(sentence)\n",
    "    left_context, right_context = get_mask_context(tokens, mask_token, window)\n",
    "    \n",
    "    # If mask token not found, return fallback.\n",
    "    if left_context is None or right_context is None:\n",
    "        return fallback_word\n",
    "    \n",
    "    key = (left_context, right_context)\n",
    "    \n",
    "    # 1. Exact match\n",
    "    if key in context_dict:\n",
    "        candidates = context_dict[key]\n",
    "        return max(candidates, key=candidates.get)\n",
    "    \n",
    "    # 2. Relaxed matching: limit search to keys that appear in left_index for tokens in left_context.\n",
    "    candidate_scores = Counter()\n",
    "    candidate_keys = set()\n",
    "    for token in left_context:\n",
    "        candidate_keys.update(left_index.get(token, set()))\n",
    "    \n",
    "    for train_key in candidate_keys:\n",
    "        train_left, train_right = train_key\n",
    "        # Compute a score: count overlapping tokens on left and right.\n",
    "        score = len(set(train_left).intersection(left_context)) + len(set(train_right).intersection(right_context))\n",
    "        if score > 0:\n",
    "            for word, freq in context_dict[train_key].items():\n",
    "                candidate_scores[word] += score * freq\n",
    "                \n",
    "    if candidate_scores:\n",
    "        return candidate_scores.most_common(1)[0][0]\n",
    "    \n",
    "    return fallback_word\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Load CSV files (update paths if needed)\n",
    "    train_file = 'train_set_f.csv'\n",
    "    test_file = 'test_set_f.csv'\n",
    "    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    \n",
    "    # Build context dictionary from training sentences.\n",
    "    # Ensure the column name 'SENTENCES' matches your CSV header.\n",
    "    train_sentences = train_df['SENTENCES'].tolist()\n",
    "    context_dict = build_context_dict(train_sentences, window=2)\n",
    "    \n",
    "    # Build the left index for faster relaxed matching.\n",
    "    left_index = build_left_index(context_dict)\n",
    "    \n",
    "    # Compute the most common word from training data as a fallback.\n",
    "    fallback_word = get_most_common_word(train_sentences)\n",
    "    print(\"Fallback word:\", fallback_word)\n",
    "    \n",
    "    # Debug: Print a few test contexts.\n",
    "    print(\"\\n--- Debug: Sample Test Contexts ---\")\n",
    "    for i in range(5):\n",
    "        sentence = test_df['MASKED SENTENCES'].iloc[i]\n",
    "        tokens = tokenize(sentence)\n",
    "        left_context, right_context = get_mask_context(tokens, mask_token=\"masked\", window=2)\n",
    "        print(f\"Sentence {i+1}: {sentence}\")\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Left Context:\", left_context)\n",
    "        print(\"Right Context:\", right_context)\n",
    "        print(\"---\")\n",
    "    \n",
    "    # Predict the masked word for each test sentence.\n",
    "    predictions = []\n",
    "    for sentence in test_df['MASKED SENTENCES']:\n",
    "        pred = predict_masked_word(sentence, context_dict, left_index, fallback_word, mask_token=\"masked\", window=2)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Debug: Print a few predictions.\n",
    "    print(\"\\n--- Debug: Sample Predictions ---\")\n",
    "    for i in range(5):\n",
    "        print(\"Test Sentence:\", test_df['MASKED SENTENCES'].iloc[i])\n",
    "        print(\"Predicted Word:\", predictions[i])\n",
    "        print(\"---\")\n",
    "    \n",
    "    # Generate and save the submission file.\n",
    "    submission_df = pd.DataFrame({\n",
    "        'IDS': test_df['IDS'],  # Ensure this matches your CSV header for identifiers.\n",
    "        'PREDICTED WORDS': predictions\n",
    "    })\n",
    "    submission_df.to_csv('sample_submission2.csv', index=False)\n",
    "    print(\"\\nSubmission file created: sample_submission2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7313ceb-a64a-4ce9-866e-000ce7b63737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
