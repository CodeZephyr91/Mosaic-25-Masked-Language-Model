{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a41d66-6065-4aa2-9df5-0a1d13cca7bc",
   "metadata": {},
   "source": [
    "## MOSAIC PS-1 (MASKED LANGUAGE MODEL DESIGNING)\n",
    "\n",
    "### NLP Pipeline followed-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f867839-58f9-4d27-90ad-5ae54319c624",
   "metadata": {},
   "source": [
    "#### 1. Text preprocessing of training as well as testing dataset\n",
    "          - Lowercasing\n",
    "          - Removal of punctuation marks\n",
    "          - Tokenization into words\n",
    "#### 2. Utilization of Google-News(300D) Word2Vec embeddings:\n",
    "          - For better semantic understanding, wide coverage and enhanced generalization\n",
    "          - loading the Google-News Word2Vec (300D) embeddings using gensim\n",
    "#### 3. Masking different parts of speech in the training data:\n",
    "The training data set containing about 50,000 sentences is subjected to masking of different parts of speech for training the model\n",
    "first 10,000 sentences adjectives are masked, next 10,000 sentences verbs, next 10,000 sentences adverbs, next 10,000 sentences nouns                   and last 10,000 sentences determiners\n",
    "                  \n",
    "            - Fallback parts of speech- Prepositions, Pronouns and Conjuctions\n",
    "#### 4. Training a Bidirectional LSTM model using combined loss evaluation metric:\n",
    "Bidirectional LSTM model used for better contextual understanding from both right and left hand sides of the masked word.\n",
    "Evaluation metric utilized for testing the model's performance is a combined loss calculated taking into account the actual mathematical            distance between the predicted embedding and the original word's embedding in the embedding space and the cosine similarity for a better                semantic context. The contribution of either to the net loss is determined by tuning the hyperparameters alpha and beta, fine tuned                     depending on the model's performance on training and cross validation set.\n",
    "\n",
    "#### 5. Final compilation of the predicted words into a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf0ba7-f441-421c-b090-f37b738cb889",
   "metadata": {},
   "source": [
    "### Text Pre-Processing-\n",
    "Removing punctuation from all the sentences in the data frame.\n",
    "Lowercasing all the sentences in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991ff6ba-c394-4e55-9a16-715ce0578346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDS</th>\n",
       "      <th>SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the capitals inaugural season was dreadful eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>there have been a few unsubstantiated reports ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>after the war he served in various positions i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>she was a dancer singer and actress long befor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>his son christopher o</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDS                                          SENTENCES\n",
       "0    1  the capitals inaugural season was dreadful eve...\n",
       "1    2  there have been a few unsubstantiated reports ...\n",
       "2    3  after the war he served in various positions i...\n",
       "3    4  she was a dancer singer and actress long befor...\n",
       "4    5                              his son christopher o"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\"\"\"Reading the csv dataset into pandas dataframe and applying lowercasing\"\"\"\n",
    "\n",
    "train_df=pd.read_csv(\"train_set_f.csv\") #Loading training data csv file as pandas dataframe\n",
    "train_df['SENTENCES']=train_df['SENTENCES'].str.lower() #Converting all sentences in the training data frame to lower case\n",
    "\n",
    "\"\"\"Removing punctuation from the sentences in the dataframe\"\"\"\n",
    "\n",
    "exclude=string.punctuation #Flexible list of punctuation in python as defined in the in-built string module\n",
    "def remove_punc(text): #Function to remove punctuation symbols(as mentioned in exclude) from the text passed as parameter\n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))\n",
    "train_df['SENTENCES']=train_df['SENTENCES'].apply(remove_punc) #The function remove_punc is applied to every value in the SENTENCES series in the dataframe\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a427468-649d-4c17-86e6-6a81b182bf61",
   "metadata": {},
   "source": [
    "***Tokenization using spacy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174302cc-fa8c-456c-9857-1585f00c7c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDS</th>\n",
       "      <th>SENTENCES</th>\n",
       "      <th>TOKENS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the capitals inaugural season was dreadful eve...</td>\n",
       "      <td>[the, capitals, inaugural, season, was, dreadf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>there have been a few unsubstantiated reports ...</td>\n",
       "      <td>[there, have, been, a, few, unsubstantiated, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>after the war he served in various positions i...</td>\n",
       "      <td>[after, the, war, he, served, in, various, pos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>she was a dancer singer and actress long befor...</td>\n",
       "      <td>[she, was, a, dancer, singer, and, actress, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>his son christopher o</td>\n",
       "      <td>[his, son, christopher, o]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDS                                          SENTENCES  \\\n",
       "0    1  the capitals inaugural season was dreadful eve...   \n",
       "1    2  there have been a few unsubstantiated reports ...   \n",
       "2    3  after the war he served in various positions i...   \n",
       "3    4  she was a dancer singer and actress long befor...   \n",
       "4    5                              his son christopher o   \n",
       "\n",
       "                                              TOKENS  \n",
       "0  [the, capitals, inaugural, season, was, dreadf...  \n",
       "1  [there, have, been, a, few, unsubstantiated, r...  \n",
       "2  [after, the, war, he, served, in, various, pos...  \n",
       "3  [she, was, a, dancer, singer, and, actress, lo...  \n",
       "4                         [his, son, christopher, o]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\"\"\"Loading the English Tokenizer model of spacy and generating the spacy doc object representing\n",
    "   the processed text after undergoing the nlp pipeline. Therafter extracting all the tokens separately and storing them into a new\n",
    "    Series in the pandas dataframe\"\"\"\n",
    "\n",
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "def token_generate(text):\n",
    "    spacy_doc=nlp(text) #Spacy doc object that represents the text which has been processed after undergoing the nlp pipeline\n",
    "    return [token.text for token in spacy_doc] #extracting each token separately\n",
    "\n",
    "train_df['TOKENS']=train_df['SENTENCES'].apply(token_generate) #creating a now series in the original dataframe to store the set of tokens corresponding to every sentence\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea034f-5a68-483d-b98c-8706b3c4c3ea",
   "metadata": {},
   "source": [
    "#### Word Embeddings:\n",
    "The pre-processed text is now to be vectorized to give meaningful number representation for the words while retaining the semantics and context of the sentence. Since Deep Learning models are mathematical implementations capable of interpretting and manipulating numbers, the vectorization is indespensable. To ensure wide coverage, better semantic understanding and enhanced generalization, pre-trained ***Google-News(300D) Word2Vec embeddings*** have been included with the help of the ***genism*** library of python and utilized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6197ba54-8232-4c0e-abc2-1f3ab891d077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'king' and 'queen': 0.6510956883430481\n",
      "Words similar to 'king': [('kings', 0.7138045430183411), ('queen', 0.6510957479476929), ('monarch', 0.6413194537162781), ('crown_prince', 0.6204220056533813), ('prince', 0.6159993410110474)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\"\"\"\n",
    "Loading the pre-trained Google News word2vec model\n",
    "The pre-trained model file (GoogleNews-vectors-negative300.bin) has been downloaded in the same directory\n",
    "\"\"\"\n",
    "\n",
    "google_news_model_path = \"GoogleNews-vectors-negative300.bin\"  # Path to the pre-trained Google News embeddings file\n",
    "w2v_model = KeyedVectors.load_word2vec_format(google_news_model_path, binary=True)\n",
    "\n",
    "# Checking similarity between two words, by example, 'king' and 'queen'\n",
    "similarity = w2v_model.similarity('king', 'queen')\n",
    "print(f\"Similarity between 'king' and 'queen': {similarity}\")\n",
    "\n",
    "#Getting the most similar words to a given specific word\n",
    "similar_words = w2v_model.most_similar('king', topn=5)\n",
    "print(f\"Words similar to 'king': {similar_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b64510-7fed-41a5-ae05-382aa7959554",
   "metadata": {},
   "source": [
    "#### Masking different Parts of Speech in the Training dataset\n",
    "The primary approach to train the BiLSTM model is to ***mimic the human interpretation of filling in a blank word in a sentence by training the model to analyze the positioning of different parts of speech in sentences individually so as to analyze which part of speech to use in what form most aptly in given unseen data where a masked word is to be predicted***. For this purpose, firstly primary parts of speech- including nouns, verbs, adverbs, adjectives and determiners are masked in sentences by dividing the dataset into 5 groups of 10,000 sentences and masking one given POS as per the order mentioned in the list(The order has been improved by hit and trial by interchanging positions of different POS to gain maximum usage of the training data). In case a given sentence does not have the designated POS, fallback POS categories have been defined in specific order of priority, which will be masked in case the main POS categories are not present in the sentence thereby ensuring maximum usage of the training data. The compiled masked sentences are stored finally in a numpy array x_train along with the actual masked words in y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97378d4d-c114-4113-b932-ca681d3bfef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total skipped sentences: 1461\n",
      "x_train shape: (48539, 40, 300)\n",
      "y_train shape: (48539, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Loading model from spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# Manual assignment of a specialized embedding indicating the presence of a masked token\n",
    "mask_embedding = np.full((300,), 9.99)\n",
    "\n",
    "main_pos_categories = [\"ADJ\", \"VERB\", \"ADV\", \"NOUN\", \"DET\"] #Primary POS categories to be masked\n",
    "fallback_categories = [\"ADP\",\"AUX\", \"PRON\", \"CCONJ\", \"SCONJ\"] #Fallback POS categories, masked in case a sentence does not contain the main pos category, for better utilization of training data\n",
    "\n",
    "num_sentences = len(train_df) #Total number of Sentences\n",
    "num_sentences_per_group = num_sentences // len(main_pos_categories) # 10,000 sentences for each category(50,000 sentences in total in the training dataset)\n",
    "\n",
    "# Function to mask a sentence\n",
    "def mask_sentence(tokens, pos_to_mask):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    sentence_embedding = []\n",
    "    target_embedding = None\n",
    "    masked = False #Ensuring exactly one word is masked per sentence\n",
    "\n",
    "    fallback_candidates = {pos: None for pos in fallback_categories}  # Store fallback word position\n",
    "\n",
    "    for idx, token in enumerate(doc):\n",
    "        word, pos = token.text, token.pos_\n",
    "\n",
    "        if not masked and pos == pos_to_mask and word in word2vec:\n",
    "            sentence_embedding.append(mask_embedding)\n",
    "            target_embedding = word2vec[word]\n",
    "            masked = True\n",
    "        else:\n",
    "            sentence_embedding.append(word2vec[word] if word in word2vec else np.zeros((300,)))\n",
    "\n",
    "        # Store fallback words if needed\n",
    "        if pos in fallback_candidates and word in word2vec and fallback_candidates[pos] is None:\n",
    "            fallback_candidates[pos] = (idx, word)\n",
    "\n",
    "    # Use fallback if no primary POS was found\n",
    "    if not masked:\n",
    "        for fallback_pos in fallback_categories:\n",
    "            if fallback_candidates[fallback_pos]:  \n",
    "                idx, fallback_word = fallback_candidates[fallback_pos]\n",
    "                sentence_embedding[idx] = mask_embedding\n",
    "                target_embedding = word2vec[fallback_word]\n",
    "                masked = True\n",
    "                break  \n",
    "\n",
    "    return (sentence_embedding, target_embedding) if masked else (None, None)\n",
    "\n",
    "\n",
    "# Process dataset\n",
    "x_train, y_train = [], []\n",
    "num_skipped = 0\n",
    "\n",
    "for i, tokens in enumerate(train_df[\"TOKENS\"]):\n",
    "    pos_to_mask = main_pos_categories[i // num_sentences_per_group]  # Assign POS category\n",
    "    sentence_emb, target_emb = mask_sentence(tokens, pos_to_mask)\n",
    "\n",
    "    if sentence_emb is not None and target_emb is not None:\n",
    "        x_train.append(sentence_emb)\n",
    "        y_train.append(target_emb)\n",
    "    else:\n",
    "        num_skipped += 1\n",
    "\n",
    "print(f\"Total skipped sentences: {num_skipped}\")\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(len(seq) for seq in x_train)\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, dtype=\"float32\", padding=\"post\")\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Final shape\n",
    "print(f\"x_train shape: {x_train.shape}\")  \n",
    "print(f\"y_train shape: {y_train.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bd1c3c-24e1-4e4a-984b-670c392fb969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Sample: [[ 8.00781250e-02  1.04980469e-01  4.98046875e-02 ...  3.66210938e-03\n",
      "   4.76074219e-02 -6.88476562e-02]\n",
      " [ 1.63085938e-01 -6.39648438e-02  8.64257812e-02 ...  2.56347656e-02\n",
      "   1.00097656e-01  6.20117188e-02]\n",
      " [ 9.98999977e+00  9.98999977e+00  9.98999977e+00 ...  9.98999977e+00\n",
      "   9.98999977e+00  9.98999977e+00]\n",
      " ...\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "Y_train Sample: [ 2.18505859e-02 -7.72094727e-03 -2.74658203e-02 -7.37304688e-02\n",
      "  1.25000000e-01 -5.11718750e-01 -2.55859375e-01 -1.15722656e-01\n",
      "  4.58984375e-02  1.82617188e-01  4.71191406e-02 -3.41796875e-02\n",
      "  1.64062500e-01  1.79443359e-02  1.16210938e-01  1.85546875e-01\n",
      "  1.88476562e-01  1.33789062e-01  8.54492188e-02  2.48046875e-01\n",
      " -1.15722656e-01  3.39843750e-01  1.63574219e-02 -5.34667969e-02\n",
      " -2.77099609e-02 -1.34765625e-01  9.03320312e-02  1.39648438e-01\n",
      "  6.93359375e-02 -4.19921875e-02 -1.74804688e-01  3.58886719e-02\n",
      "  1.64794922e-02  1.28906250e-01  2.89062500e-01  4.22363281e-02\n",
      " -6.44531250e-02 -3.61328125e-01 -2.33154297e-02 -1.02539062e-01\n",
      " -5.54199219e-02 -8.59375000e-02  1.71875000e-01  4.23828125e-01\n",
      "  1.94091797e-02 -1.70898438e-01 -1.19140625e-01 -8.44726562e-02\n",
      " -1.20117188e-01  2.59765625e-01 -5.05371094e-02  5.07812500e-02\n",
      " -1.26953125e-02  7.12890625e-02  1.87500000e-01 -5.61523438e-02\n",
      " -1.47460938e-01 -2.59765625e-01  1.01562500e-01 -1.22070312e-01\n",
      " -8.00781250e-02  6.83593750e-02  4.83398438e-02 -1.17187500e-01\n",
      " -3.19824219e-02  1.03027344e-01 -2.29492188e-02  1.16210938e-01\n",
      "  3.20312500e-01  2.77343750e-01  3.12500000e-02 -1.56250000e-01\n",
      "  1.90429688e-01 -1.82617188e-01  1.56250000e-01 -5.83496094e-02\n",
      "  1.04003906e-01  1.65039062e-01  1.19018555e-02 -5.76171875e-02\n",
      "  2.17773438e-01 -1.52343750e-01 -3.02734375e-01 -3.49121094e-02\n",
      " -2.59765625e-01 -1.43554688e-01 -2.32421875e-01 -1.07421875e-01\n",
      "  7.71484375e-02  3.55468750e-01  7.86132812e-02 -2.20703125e-01\n",
      " -3.96728516e-03 -3.75000000e-01 -1.09863281e-01  4.22363281e-02\n",
      " -1.04003906e-01  1.14257812e-01 -1.48437500e-01  1.93359375e-01\n",
      "  1.56250000e-01 -5.15136719e-02  1.05468750e-01  1.05957031e-01\n",
      " -2.09960938e-01  2.19726562e-01  2.81982422e-02 -8.65936279e-04\n",
      " -9.57031250e-02  2.31445312e-01  1.34765625e-01  1.12304688e-01\n",
      "  6.34765625e-02  1.17187500e-01  2.30468750e-01  2.83203125e-02\n",
      "  1.21582031e-01 -1.90429688e-01  9.86328125e-02 -1.43554688e-01\n",
      " -1.42578125e-01  1.70898438e-02  9.52148438e-02 -2.51464844e-02\n",
      "  6.34765625e-03  6.49414062e-02 -3.56445312e-02  5.93261719e-02\n",
      "  1.15722656e-01  7.42187500e-02 -1.74804688e-01 -1.69677734e-02\n",
      " -1.33789062e-01 -2.56347656e-02 -1.35742188e-01  9.96093750e-02\n",
      " -1.65039062e-01  6.44531250e-02 -1.49536133e-02  2.22656250e-01\n",
      " -2.05078125e-01  1.05468750e-01  1.19140625e-01 -2.87109375e-01\n",
      " -1.29882812e-01 -3.88183594e-02 -3.93066406e-02 -8.83789062e-02\n",
      "  2.57812500e-01 -1.02539062e-01 -1.30859375e-01  1.41601562e-01\n",
      "  6.59179688e-02 -6.68945312e-02  1.00585938e-01 -1.58203125e-01\n",
      " -6.29882812e-02 -2.02148438e-01 -7.42187500e-02 -2.08984375e-01\n",
      " -1.53320312e-01 -2.51953125e-01  2.75878906e-02 -1.61132812e-02\n",
      "  8.54492188e-02  7.17163086e-03  2.29492188e-01 -1.62109375e-01\n",
      " -1.24023438e-01 -4.29687500e-02 -3.19824219e-02  5.05371094e-02\n",
      "  4.02832031e-02 -4.56542969e-02 -1.03149414e-02 -1.69921875e-01\n",
      "  1.02539062e-01 -8.69140625e-02 -2.41210938e-01 -7.56835938e-02\n",
      " -6.80541992e-03  3.19824219e-02  6.83593750e-02  8.30078125e-02\n",
      " -8.77380371e-04  1.39648438e-01  2.35351562e-01 -8.64257812e-02\n",
      " -5.15136719e-02  3.14453125e-01  1.20117188e-01  1.04003906e-01\n",
      " -2.50244141e-02  8.00781250e-02 -3.69140625e-01 -7.91015625e-02\n",
      "  2.65625000e-01  1.09863281e-01  1.22558594e-01 -1.07421875e-01\n",
      "  1.70898438e-02 -3.51562500e-02  4.83398438e-02 -1.51367188e-01\n",
      "  2.45117188e-01 -1.67968750e-01  5.51757812e-02  4.56542969e-02\n",
      "  1.92871094e-02  7.61718750e-02 -2.98828125e-01 -4.98046875e-02\n",
      "  6.12792969e-02 -8.17871094e-03  4.58984375e-02 -6.78710938e-02\n",
      "  4.90722656e-02 -2.10937500e-01  5.90820312e-02 -1.15234375e-01\n",
      " -2.35351562e-01  1.93359375e-01 -1.08398438e-01  7.81250000e-02\n",
      "  1.25976562e-01 -6.78710938e-02  8.15429688e-02  3.41796875e-02\n",
      "  4.37011719e-02 -7.22656250e-02 -2.08740234e-02  1.82617188e-01\n",
      " -7.56835938e-02  1.41601562e-01  9.91210938e-02 -9.52148438e-02\n",
      " -1.57470703e-02  6.73828125e-02 -1.93359375e-01 -2.44140625e-01\n",
      " -2.17773438e-01  3.91006470e-05 -8.39843750e-02 -1.69921875e-01\n",
      " -3.85742188e-02  5.85937500e-03  7.61718750e-02  1.04003906e-01\n",
      "  1.49414062e-01 -3.71093750e-02 -8.39843750e-02 -7.66601562e-02\n",
      " -4.22363281e-02 -2.71484375e-01  6.74438477e-03  1.83593750e-01\n",
      "  2.06054688e-01 -3.17382812e-02 -3.94531250e-01 -4.10156250e-02\n",
      "  9.37500000e-02  3.06640625e-01  1.58203125e-01 -1.21093750e-01\n",
      "  9.27734375e-02 -7.61718750e-02  5.88378906e-02 -1.95312500e-01\n",
      " -3.95507812e-02 -9.17968750e-02 -1.51367188e-01 -3.80859375e-02\n",
      "  1.61132812e-01 -2.51464844e-02  1.41601562e-01 -7.37304688e-02\n",
      " -1.34277344e-02  2.11914062e-01 -3.04687500e-01  9.27734375e-02\n",
      "  8.34960938e-02 -1.64794922e-02 -1.94335938e-01  4.61425781e-02\n",
      "  5.68847656e-02  3.51562500e-02 -2.26562500e-01 -1.21582031e-01\n",
      "  6.65283203e-03 -1.53320312e-01 -7.93457031e-03 -3.20312500e-01\n",
      " -1.48437500e-01 -2.66113281e-02  1.52343750e-01 -1.80664062e-01\n",
      "  1.23046875e-01  1.62109375e-01 -2.55859375e-01 -2.03125000e-01]\n",
      "--------------------------------------------------\n",
      "X_train Sample: [[ 0.09423828 -0.02282715  0.05224609 ... -0.046875    0.16113281\n",
      "  -0.19921875]\n",
      " [-0.13964844 -0.03466797 -0.05371094 ... -0.11816406 -0.00245667\n",
      "  -0.07226562]\n",
      " [-0.10107422  0.0177002   0.01470947 ... -0.08349609  0.00558472\n",
      "  -0.01782227]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Y_train Sample: [ 0.13867188  0.02941895 -0.18652344  0.15039062  0.0703125  -0.30664062\n",
      " -0.09765625 -0.07421875  0.08251953  0.08935547 -0.23730469 -0.18164062\n",
      " -0.05834961  0.17675781 -0.13964844  0.109375    0.11914062  0.17675781\n",
      " -0.2578125   0.00262451 -0.02526855  0.02612305 -0.03833008 -0.06982422\n",
      " -0.03857422  0.17773438 -0.01470947  0.02575684 -0.08837891 -0.03125\n",
      " -0.24609375  0.02893066  0.09179688 -0.12255859 -0.2265625  -0.04125977\n",
      "  0.13476562 -0.13671875  0.12792969 -0.08056641  0.25390625 -0.07568359\n",
      "  0.125       0.13476562 -0.05273438  0.13476562 -0.17773438 -0.13085938\n",
      " -0.04296875 -0.07568359 -0.07275391 -0.13769531 -0.26367188 -0.06225586\n",
      " -0.13867188  0.11914062  0.1328125  -0.05981445  0.13085938 -0.09912109\n",
      " -0.14648438  0.15722656  0.03222656 -0.03930664 -0.08154297 -0.01647949\n",
      " -0.10546875 -0.01867676 -0.05078125  0.00897217 -0.06298828  0.12890625\n",
      " -0.03393555  0.05883789 -0.30859375  0.01104736 -0.02905273  0.02929688\n",
      "  0.03088379 -0.00811768  0.14746094 -0.04272461 -0.07373047 -0.06347656\n",
      " -0.22167969 -0.10351562  0.00921631 -0.17773438  0.00430298  0.12890625\n",
      "  0.11523438  0.23632812 -0.19433594  0.09472656 -0.22363281  0.03222656\n",
      "  0.0078125   0.09228516 -0.05883789  0.11523438 -0.01403809  0.07128906\n",
      "  0.28320312 -0.04370117 -0.01953125 -0.24414062  0.06542969  0.11621094\n",
      "  0.140625   -0.18066406 -0.20898438  0.15332031  0.16015625 -0.17285156\n",
      "  0.05419922 -0.13183594  0.07177734  0.09667969  0.14648438  0.07080078\n",
      " -0.12060547  0.08056641 -0.17871094 -0.09179688  0.07763672 -0.06225586\n",
      " -0.15820312  0.07519531 -0.03466797  0.12695312  0.09082031 -0.13964844\n",
      "  0.08398438 -0.03466797 -0.00952148 -0.08447266  0.10693359  0.09375\n",
      " -0.03662109  0.09179688 -0.06298828 -0.26171875  0.140625    0.09521484\n",
      "  0.20214844  0.12060547 -0.01696777  0.07519531 -0.08349609 -0.13085938\n",
      "  0.05444336  0.03930664 -0.06445312  0.0144043  -0.05566406  0.05810547\n",
      " -0.02453613  0.08203125 -0.15136719  0.22753906 -0.06738281  0.13867188\n",
      "  0.07519531 -0.04101562  0.15039062 -0.10449219 -0.02648926  0.07714844\n",
      "  0.02111816 -0.21191406 -0.23046875  0.24707031 -0.2578125  -0.25585938\n",
      "  0.05541992 -0.10449219 -0.25        0.0300293   0.11962891  0.06982422\n",
      " -0.00466919 -0.07177734  0.01556396  0.23242188  0.06347656  0.10449219\n",
      " -0.0402832   0.01092529 -0.04516602 -0.06298828  0.11816406  0.02734375\n",
      "  0.12890625 -0.12207031 -0.26367188 -0.03295898  0.0279541   0.03808594\n",
      " -0.140625    0.01080322 -0.08984375  0.21875     0.08642578 -0.05688477\n",
      "  0.01635742 -0.11035156 -0.15429688 -0.0213623  -0.06689453  0.0246582\n",
      "  0.03198242 -0.1328125   0.01757812 -0.09228516  0.12597656  0.2578125\n",
      "  0.17773438 -0.0456543  -0.20019531  0.13964844 -0.15722656 -0.02868652\n",
      " -0.10839844 -0.09619141  0.16210938 -0.2109375   0.00183868  0.07910156\n",
      " -0.11132812  0.06201172 -0.01171875 -0.11279297  0.2109375  -0.02783203\n",
      "  0.2734375  -0.02160645  0.03759766 -0.11621094 -0.08789062 -0.06225586\n",
      "  0.00811768  0.20410156  0.14257812 -0.30664062  0.17871094 -0.06103516\n",
      "  0.0378418   0.01239014  0.01062012  0.09082031 -0.06298828  0.07568359\n",
      " -0.04296875  0.01623535  0.125       0.1796875   0.10009766  0.12158203\n",
      " -0.23339844 -0.31054688  0.0234375  -0.22070312 -0.09570312 -0.09570312\n",
      "  0.14941406  0.11572266 -0.12304688  0.04956055 -0.11816406 -0.0324707\n",
      "  0.35742188  0.19042969  0.08544922  0.046875   -0.03613281 -0.18945312\n",
      " -0.10009766 -0.06640625 -0.1171875  -0.10693359  0.09619141 -0.02380371\n",
      "  0.22363281  0.03979492  0.08496094  0.04858398 -0.14355469  0.0402832\n",
      " -0.1953125   0.3359375  -0.00267029  0.08203125 -0.18261719 -0.15429688\n",
      "  0.01257324  0.08203125  0.11816406  0.10058594 -0.09912109 -0.03173828]\n",
      "--------------------------------------------------\n",
      "X_train Sample: [[ 9.3750000e-02 -5.0048828e-02  1.7480469e-01 ...  8.3618164e-03\n",
      "   6.3964844e-02 -2.6367188e-02]\n",
      " [ 8.0078125e-02  1.0498047e-01  4.9804688e-02 ...  3.6621094e-03\n",
      "   4.7607422e-02 -6.8847656e-02]\n",
      " [ 3.3984375e-01  3.0468750e-01  9.8632812e-02 ...  1.5258789e-04\n",
      "   8.2031250e-02  2.0898438e-01]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]]\n",
      "Y_train Sample: [ 3.49121094e-02  4.27246094e-02 -1.45507812e-01  3.58886719e-02\n",
      "  1.32560730e-04 -8.64257812e-02 -5.73730469e-02  1.34765625e-01\n",
      "  7.27539062e-02 -1.00097656e-01 -4.88281250e-02 -1.38671875e-01\n",
      " -1.25000000e-01  2.67578125e-01 -3.75000000e-01  4.58984375e-02\n",
      " -2.17773438e-01  2.50000000e-01 -2.53906250e-01  8.48388672e-03\n",
      " -5.51757812e-02 -6.34765625e-02 -2.41210938e-01 -6.29882812e-02\n",
      " -5.46875000e-02  5.90820312e-02 -2.27539062e-01 -6.83593750e-02\n",
      "  2.47192383e-03 -1.62109375e-01 -1.92382812e-01 -2.00195312e-01\n",
      "  7.03125000e-02  5.54199219e-02 -1.25000000e-01 -1.15234375e-01\n",
      "  1.36718750e-01 -2.57812500e-01  2.16064453e-02 -1.79687500e-01\n",
      "  2.92968750e-01 -1.81884766e-02 -7.22656250e-02 -3.76892090e-03\n",
      " -1.01074219e-01 -6.25000000e-02 -1.15722656e-01 -7.76367188e-02\n",
      " -6.68945312e-02  1.93023682e-03  9.47265625e-02  1.93359375e-01\n",
      " -1.66015625e-01  8.25195312e-02 -2.79296875e-01  6.34765625e-02\n",
      "  3.02734375e-02 -2.79296875e-01  2.05078125e-01  4.51660156e-02\n",
      " -1.40625000e-01  1.89453125e-01  1.01562500e-01 -2.57568359e-02\n",
      "  7.47070312e-02 -9.81445312e-02 -1.64062500e-01  2.12890625e-01\n",
      "  4.88281250e-02  6.92749023e-03  2.67333984e-02  4.15039062e-02\n",
      "  1.42578125e-01  1.14257812e-01 -9.08203125e-02 -4.33349609e-03\n",
      " -3.61328125e-02  5.27343750e-02 -8.74023438e-02  1.18408203e-02\n",
      "  3.34472656e-02 -7.91015625e-02 -6.12792969e-02 -2.63671875e-01\n",
      "  5.05371094e-02 -8.74023438e-02 -2.10937500e-01 -6.29882812e-02\n",
      " -2.48046875e-01  1.85546875e-01  1.58203125e-01  1.02050781e-01\n",
      " -2.57568359e-02 -9.42382812e-02  7.61718750e-02  1.59179688e-01\n",
      "  1.23535156e-01 -1.46484375e-01  1.31835938e-01  2.02148438e-01\n",
      "  2.78320312e-02  2.58789062e-02  1.52587891e-02 -1.43554688e-01\n",
      "  6.10351562e-02  7.71484375e-02 -8.78906250e-02  1.03149414e-02\n",
      "  4.00390625e-01 -6.34765625e-02 -2.37304688e-01 -1.05468750e-01\n",
      " -3.49426270e-03 -1.12792969e-01  1.12792969e-01  1.17797852e-02\n",
      "  1.24511719e-02 -8.25195312e-02  3.18359375e-01 -4.24804688e-02\n",
      " -1.00097656e-01  4.17480469e-02 -1.80664062e-01 -4.61425781e-02\n",
      " -6.01196289e-03 -1.04003906e-01 -8.83789062e-02  1.11694336e-02\n",
      " -3.41796875e-02  1.92382812e-01  3.88183594e-02  1.68457031e-02\n",
      " -1.35742188e-01 -3.26171875e-01 -9.17968750e-02  6.03027344e-02\n",
      " -3.12500000e-02 -2.15820312e-01  5.63964844e-02  2.22167969e-02\n",
      " -7.71484375e-02  2.23632812e-01 -1.86767578e-02  2.28515625e-01\n",
      "  2.13867188e-01  1.52343750e-01 -4.15039062e-02  1.20117188e-01\n",
      " -7.51953125e-02  1.46484375e-01 -4.32128906e-02  1.06933594e-01\n",
      " -1.46484375e-01 -1.40380859e-02 -1.57226562e-01 -1.16210938e-01\n",
      "  1.07421875e-02 -6.88476562e-02  3.02734375e-02  3.34472656e-02\n",
      " -8.91113281e-03 -5.06591797e-03 -1.62109375e-01  2.91748047e-02\n",
      "  2.36328125e-01  1.20605469e-01 -2.42919922e-02 -1.13769531e-01\n",
      "  1.86523438e-01 -5.56640625e-02 -2.23632812e-01 -1.06811523e-02\n",
      "  4.61425781e-02 -9.76562500e-02  1.05957031e-01 -1.66992188e-01\n",
      " -1.23046875e-01 -3.16406250e-01  4.51660156e-02 -7.51953125e-02\n",
      " -1.69921875e-01 -1.01074219e-01 -1.09375000e-01  3.49121094e-02\n",
      "  1.16699219e-01  5.55419922e-03 -1.00097656e-01  1.13281250e-01\n",
      "  5.93261719e-02 -9.96093750e-02  1.55273438e-01 -5.10253906e-02\n",
      "  2.16064453e-02 -2.87109375e-01 -9.22851562e-02 -1.38671875e-01\n",
      "  3.24707031e-02 -8.00781250e-02  5.66406250e-02 -1.21093750e-01\n",
      "  1.74804688e-01  1.91406250e-01  6.22558594e-02 -1.80664062e-01\n",
      " -4.23431396e-04 -1.08398438e-01 -6.25000000e-02 -1.87988281e-02\n",
      " -2.90527344e-02  3.35937500e-01  5.88378906e-02 -2.87109375e-01\n",
      " -7.22656250e-02 -7.61718750e-02  3.24218750e-01 -9.08203125e-02\n",
      "  6.29882812e-02  1.54296875e-01 -1.04492188e-01 -1.45507812e-01\n",
      " -6.15234375e-02 -2.14843750e-01 -1.13281250e-01 -1.50390625e-01\n",
      "  8.10546875e-02 -6.95800781e-03 -2.00195312e-01  5.83496094e-02\n",
      "  1.05590820e-02  6.29882812e-02  1.92382812e-01  6.39648438e-02\n",
      " -5.71289062e-02  4.71191406e-02  9.42382812e-02 -7.08007812e-02\n",
      "  2.05078125e-01  3.36914062e-02  2.38037109e-02  2.49023438e-01\n",
      "  4.00390625e-02  9.57031250e-02 -2.44140625e-02 -3.01513672e-02\n",
      "  5.12695312e-02  4.54101562e-02 -6.44531250e-02 -1.50390625e-01\n",
      " -1.65039062e-01  3.58886719e-02  7.47070312e-02  2.81250000e-01\n",
      " -6.88476562e-02 -2.07031250e-01  1.89453125e-01 -1.32812500e-01\n",
      " -2.42919922e-02 -1.04003906e-01  3.32031250e-02 -1.67968750e-01\n",
      "  1.30859375e-01 -2.51464844e-02 -1.06933594e-01 -2.33459473e-03\n",
      "  1.27929688e-01 -4.85229492e-03 -1.23046875e-01  1.38671875e-01\n",
      " -7.37304688e-02  1.56250000e-02  1.23046875e-01 -1.20239258e-02\n",
      " -2.22167969e-02  6.78710938e-02  2.17773438e-01 -1.04492188e-01\n",
      " -8.34960938e-02 -1.08398438e-01 -1.62109375e-01 -4.60815430e-03\n",
      " -2.81982422e-02  2.27050781e-02  7.86132812e-02 -9.03320312e-02\n",
      "  1.31225586e-02  1.16210938e-01 -1.28906250e-01  6.34765625e-02\n",
      "  1.06933594e-01  2.77343750e-01 -1.00585938e-01  1.08032227e-02\n",
      " -9.91210938e-02 -1.33789062e-01 -1.77734375e-01  1.35742188e-01\n",
      "  1.30859375e-01  5.76171875e-02 -1.99218750e-01  4.56542969e-02]\n",
      "--------------------------------------------------\n",
      "X_train Sample: [[ 0.08837891 -0.03173828 -0.12255859 ...  0.0234375   0.24804688\n",
      "  -0.11767578]\n",
      " [ 0.02600098 -0.00189209  0.18554688 ... -0.12158203  0.22167969\n",
      "  -0.02197266]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Y_train Sample: [ 4.54101562e-02 -4.71191406e-02  6.73828125e-02 -4.58984375e-02\n",
      "  2.79541016e-02 -1.78222656e-02 -7.37304688e-02 -2.09960938e-02\n",
      "  1.36718750e-01 -6.83593750e-02 -5.68847656e-02 -3.02734375e-01\n",
      "  4.10156250e-02  1.32812500e-01 -1.77734375e-01  9.27734375e-02\n",
      "  2.04101562e-01  6.73828125e-02  2.98828125e-01 -3.41796875e-02\n",
      " -5.98144531e-02 -7.91015625e-02 -1.65039062e-01 -4.76074219e-02\n",
      "  1.48437500e-01  4.51660156e-02  1.36718750e-01 -1.89208984e-02\n",
      "  2.12890625e-01  1.99218750e-01 -1.20117188e-01  9.61914062e-02\n",
      " -1.58203125e-01  4.00390625e-02 -1.21093750e-01  3.61633301e-03\n",
      " -3.56445312e-02 -2.38281250e-01 -7.36236572e-04  5.39550781e-02\n",
      "  5.85937500e-02 -2.75390625e-01  2.31445312e-01 -9.81445312e-02\n",
      " -3.22265625e-02  1.98242188e-01 -7.56835938e-02 -8.44726562e-02\n",
      "  8.54492188e-02  1.68945312e-01  4.32128906e-02  1.04370117e-02\n",
      " -4.98046875e-02  8.20312500e-02 -1.83593750e-01  1.01562500e-01\n",
      " -2.99072266e-02 -8.83789062e-02  6.49414062e-02 -2.30468750e-01\n",
      " -2.83203125e-01 -9.96093750e-02 -5.05371094e-02 -1.44531250e-01\n",
      "  2.68554688e-02  1.89208984e-03  7.22656250e-02  1.85546875e-01\n",
      " -1.40625000e-01  1.78710938e-01 -6.64062500e-02  1.22680664e-02\n",
      " -2.55126953e-02  1.10351562e-01 -9.71679688e-02 -1.23535156e-01\n",
      "  1.76757812e-01  2.46093750e-01 -2.92968750e-02  1.46484375e-01\n",
      " -1.54296875e-01  2.47802734e-02 -1.26953125e-02 -9.17968750e-02\n",
      "  7.51953125e-02 -1.12304688e-01 -9.94873047e-03  7.12890625e-02\n",
      "  1.49414062e-01  8.30078125e-02  1.39770508e-02  2.31445312e-01\n",
      " -3.93066406e-02 -1.58691406e-02  3.51562500e-02 -2.92968750e-01\n",
      " -3.08837891e-02 -7.76367188e-02  1.56250000e-02  1.15356445e-02\n",
      "  1.16210938e-01 -1.25976562e-01  9.57031250e-02  3.27148438e-02\n",
      " -1.19628906e-01 -1.61132812e-01  1.61132812e-02 -2.55126953e-02\n",
      " -5.44433594e-02 -1.30004883e-02  2.08007812e-01  1.84570312e-01\n",
      "  1.11816406e-01 -1.19628906e-01 -2.46582031e-02  1.15722656e-01\n",
      "  3.49121094e-02  7.61718750e-02 -4.66308594e-02  2.11914062e-01\n",
      " -5.27343750e-02 -8.54492188e-02 -4.76074219e-02  7.42187500e-02\n",
      "  5.12695312e-02 -5.76171875e-02  7.47070312e-02  4.76074219e-02\n",
      " -4.63867188e-02  8.88671875e-02  7.66601562e-02 -1.59179688e-01\n",
      " -1.87500000e-01 -1.88476562e-01 -5.61523438e-02 -4.49218750e-02\n",
      " -3.95507812e-02 -9.91210938e-02 -1.17187500e-01  1.64062500e-01\n",
      " -1.92871094e-02 -8.20312500e-02  8.74023438e-02 -8.74023438e-02\n",
      "  1.40991211e-02  1.27929688e-01  6.53076172e-03 -1.09375000e-01\n",
      " -9.13085938e-02  2.85644531e-02  5.27343750e-02  1.66015625e-01\n",
      " -2.73437500e-02  5.39550781e-02 -1.47094727e-02 -1.35742188e-01\n",
      " -1.90429688e-01 -6.00585938e-02 -1.11328125e-01  3.17382812e-02\n",
      " -7.03125000e-02  2.09960938e-02  1.16210938e-01  7.51953125e-02\n",
      "  1.18652344e-01 -6.10351562e-03  2.31933594e-02  9.17968750e-02\n",
      "  9.08203125e-02 -6.34765625e-02 -1.90734863e-03 -8.78906250e-02\n",
      " -1.78710938e-01  2.51464844e-02 -1.00585938e-01 -7.47070312e-02\n",
      "  2.53906250e-02  6.44531250e-02  5.95703125e-02 -4.12597656e-02\n",
      " -8.00781250e-02 -9.37500000e-02 -1.52343750e-01  2.17773438e-01\n",
      "  3.95507812e-02 -9.15527344e-03 -1.07421875e-02  1.37695312e-01\n",
      "  8.34960938e-02  3.83300781e-02  8.34960938e-02  7.47070312e-02\n",
      "  3.80859375e-02  1.59179688e-01 -1.64062500e-01 -1.63574219e-02\n",
      "  7.22656250e-02 -5.95703125e-02  4.17480469e-02 -1.12304688e-01\n",
      "  6.34765625e-02 -9.27734375e-02 -2.46582031e-02  6.78710938e-02\n",
      "  1.68945312e-01 -2.03125000e-01  6.98242188e-02  2.51464844e-02\n",
      "  5.90820312e-02 -4.85839844e-02 -3.22265625e-02 -3.71093750e-02\n",
      " -5.71289062e-02 -1.03149414e-02 -1.10351562e-01 -1.04370117e-02\n",
      " -6.07910156e-02  2.27355957e-03 -9.37500000e-02  1.98242188e-01\n",
      " -1.31225586e-03  1.49414062e-01 -3.01513672e-02 -7.32421875e-02\n",
      "  1.56250000e-01 -1.74804688e-01 -4.29687500e-02  1.30859375e-01\n",
      "  4.98046875e-02 -2.96630859e-02  1.01562500e-01 -2.16796875e-01\n",
      " -7.27539062e-02 -3.56445312e-02 -7.17773438e-02  3.29971313e-04\n",
      "  8.88671875e-02 -1.34887695e-02 -8.85009766e-03  4.05273438e-02\n",
      " -3.95507812e-02 -5.54199219e-02  3.80859375e-02 -1.22558594e-01\n",
      "  3.75976562e-02  7.22656250e-02  9.61914062e-02  9.42382812e-02\n",
      "  2.12890625e-01 -3.00292969e-02  1.92871094e-02 -1.42822266e-02\n",
      "  2.95410156e-02 -9.15527344e-04  5.59082031e-02 -1.13281250e-01\n",
      "  6.34765625e-02  8.54492188e-02  2.03857422e-02 -1.12304688e-01\n",
      " -2.08740234e-02  7.41577148e-03 -6.78710938e-02 -3.34472656e-02\n",
      "  5.92041016e-03  1.04492188e-01 -2.38037109e-03  1.82617188e-01\n",
      "  1.20849609e-02 -6.25000000e-02  1.81640625e-01  3.63769531e-02\n",
      "  1.24023438e-01  1.07421875e-01  8.49609375e-02 -3.45703125e-01\n",
      " -1.71875000e-01 -1.84570312e-01  6.03027344e-02 -1.12792969e-01\n",
      "  1.15722656e-01 -6.10351562e-02 -1.68945312e-01  1.62109375e-01\n",
      "  2.51464844e-02  7.95898438e-02 -1.58203125e-01 -1.43554688e-01\n",
      "  7.51953125e-02  1.70898438e-01  4.31060791e-04 -5.32226562e-02\n",
      " -3.19824219e-02  1.17675781e-01 -9.66796875e-02  8.00781250e-02\n",
      " -1.50390625e-01 -1.19140625e-01 -1.00585938e-01 -1.25976562e-01]\n",
      "--------------------------------------------------\n",
      "X_train Sample: [[ 9.99        9.99        9.99       ...  9.99        9.99\n",
      "   9.99      ]\n",
      " [ 0.10791016 -0.0300293   0.03320312 ... -0.21972656 -0.04882812\n",
      "   0.17578125]\n",
      " [-0.23242188  0.21484375 -0.03588867 ... -0.24902344 -0.28125\n",
      "   0.16699219]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n",
      "Y_train Sample: [ 3.18359375e-01  1.76757812e-01  5.85937500e-02 -1.07421875e-02\n",
      " -9.39941406e-03 -4.85839844e-02 -7.22656250e-02 -1.72851562e-01\n",
      " -2.41699219e-02 -1.69921875e-01  3.14941406e-02 -1.39648438e-01\n",
      " -8.88824463e-04  1.77734375e-01 -3.26171875e-01  4.78515625e-02\n",
      "  1.94335938e-01  2.03125000e-01  9.61914062e-02  9.03320312e-03\n",
      " -1.03027344e-01  1.64062500e-01 -6.03027344e-02  7.37304688e-02\n",
      " -1.58203125e-01 -1.97265625e-01 -7.86132812e-02  7.12890625e-02\n",
      "  1.41601562e-01  1.08398438e-01  1.60156250e-01 -1.90429688e-02\n",
      " -8.11767578e-03  1.40625000e-01  4.24804688e-02 -5.00488281e-02\n",
      "  1.49414062e-01  6.64062500e-02 -9.72747803e-04 -1.17187500e-01\n",
      "  1.01562500e-01 -1.03027344e-01  2.32421875e-01  6.20117188e-02\n",
      "  1.70898438e-01  1.30859375e-01 -1.10839844e-01  1.03027344e-01\n",
      "  2.43164062e-01  7.76367188e-02  1.73828125e-01  1.02050781e-01\n",
      "  1.28173828e-02 -7.47680664e-04 -6.29882812e-02  1.32812500e-01\n",
      " -1.37695312e-01 -1.34765625e-01  3.88183594e-02 -9.66796875e-02\n",
      "  6.83593750e-02  1.45507812e-01  1.13769531e-01 -5.27343750e-02\n",
      "  6.64062500e-02  1.36718750e-01  4.61425781e-02 -2.50000000e-01\n",
      " -8.54492188e-02  1.79687500e-01  1.58203125e-01 -3.14941406e-02\n",
      " -1.19628906e-02  1.08398438e-01 -7.95898438e-02 -1.23535156e-01\n",
      "  3.73535156e-02  9.61914062e-02  1.15234375e-01 -1.21459961e-02\n",
      " -7.65991211e-03 -2.50000000e-01 -4.05273438e-02 -8.30078125e-02\n",
      "  8.69140625e-02  1.57470703e-02 -4.63867188e-02  1.64794922e-02\n",
      " -2.02148438e-01  1.38671875e-01 -5.51757812e-02  4.37011719e-02\n",
      "  8.93554688e-02  4.91333008e-03 -3.01513672e-02  2.97851562e-02\n",
      "  5.46875000e-02  1.72119141e-02 -1.17797852e-02 -4.10156250e-02\n",
      " -5.98144531e-02 -1.42578125e-01 -1.02050781e-01 -1.53320312e-01\n",
      "  1.80664062e-02 -2.68554688e-02  1.52343750e-01  2.28515625e-01\n",
      "  8.78906250e-02 -1.59179688e-01 -1.14257812e-01  7.72476196e-05\n",
      " -2.07519531e-02  3.19824219e-02 -1.20849609e-02 -6.88476562e-02\n",
      " -1.36718750e-01 -1.31835938e-01  9.86328125e-02  5.81054688e-02\n",
      " -1.96289062e-01 -4.46777344e-02  2.28271484e-02  4.80957031e-02\n",
      " -1.41601562e-01 -2.96875000e-01  3.01513672e-02 -8.44726562e-02\n",
      " -1.31835938e-01 -2.08740234e-02 -2.05078125e-01 -1.96533203e-02\n",
      " -4.19921875e-02 -4.10156250e-02 -1.13769531e-01 -3.93066406e-02\n",
      " -1.91406250e-01  1.76757812e-01 -4.41894531e-02  2.08984375e-01\n",
      "  6.78710938e-02  8.88671875e-02 -1.03515625e-01 -5.34667969e-02\n",
      " -3.54003906e-03  1.63574219e-02 -5.66406250e-02  1.57470703e-02\n",
      " -3.75976562e-02  1.56250000e-01  1.16699219e-01  3.68652344e-02\n",
      " -6.64062500e-02  4.29687500e-02 -7.86132812e-02  5.37109375e-02\n",
      " -2.68554688e-02 -8.30078125e-02  1.80664062e-01 -8.34960938e-02\n",
      " -3.07617188e-02 -6.68945312e-02  1.55273438e-01  2.13867188e-01\n",
      " -1.59179688e-01 -9.76562500e-02  1.31835938e-01 -1.03515625e-01\n",
      "  1.42578125e-01 -1.62109375e-01  1.79443359e-02 -1.33789062e-01\n",
      " -1.86523438e-01 -2.09960938e-01 -1.62109375e-01  5.78613281e-02\n",
      "  2.65502930e-03 -1.10351562e-01  3.03955078e-02  4.76074219e-02\n",
      " -6.20117188e-02  2.01171875e-01 -4.58984375e-02 -6.22558594e-02\n",
      " -1.07910156e-01  9.61914062e-02 -7.86132812e-02  4.58984375e-02\n",
      "  2.50000000e-01 -9.81445312e-02  4.19921875e-02  1.96289062e-01\n",
      "  1.83105469e-02  1.19628906e-01 -1.16210938e-01 -3.11279297e-02\n",
      " -1.40625000e-01  1.72119141e-02 -3.29589844e-02 -1.78710938e-01\n",
      " -2.89062500e-01 -1.54296875e-01  4.93164062e-02  7.03125000e-02\n",
      "  2.49023438e-01 -2.06298828e-02 -3.83300781e-02  1.57226562e-01\n",
      "  1.77001953e-02 -7.76367188e-02 -7.37304688e-02 -9.61914062e-02\n",
      " -1.19018555e-02 -1.12304688e-01 -1.29882812e-01 -2.60009766e-02\n",
      "  3.22265625e-02  5.83496094e-02 -3.28125000e-01  7.37304688e-02\n",
      "  5.63964844e-02 -2.58789062e-02  2.46582031e-02 -1.18652344e-01\n",
      "  3.26171875e-01 -9.81445312e-02 -1.28906250e-01  1.66015625e-01\n",
      "  1.84570312e-01 -1.02050781e-01  1.73828125e-01 -1.88476562e-01\n",
      "  8.25195312e-02 -2.67028809e-05  3.17382812e-02 -5.61523438e-03\n",
      " -1.49414062e-01 -6.03027344e-02 -1.85546875e-01 -1.31835938e-01\n",
      " -4.02832031e-02 -1.11816406e-01 -1.23901367e-02 -1.13281250e-01\n",
      "  2.96630859e-02  5.39550781e-02 -1.58691406e-02  8.05664062e-02\n",
      "  6.54296875e-02  8.74023438e-02  2.53906250e-02 -1.07421875e-01\n",
      " -1.09375000e-01  5.12695312e-02  4.15039062e-02  4.07714844e-02\n",
      "  1.54296875e-01 -6.83593750e-02 -1.21093750e-01  1.02050781e-01\n",
      "  7.01904297e-03  1.26953125e-01 -1.19140625e-01 -2.01416016e-02\n",
      "  3.45703125e-01  2.26562500e-01 -2.35595703e-02  6.98242188e-02\n",
      " -1.00708008e-02 -7.37304688e-02 -6.40869141e-03 -1.36108398e-02\n",
      " -3.63769531e-02 -1.20117188e-01 -1.07910156e-01 -6.73828125e-02\n",
      "  1.13769531e-01 -6.15234375e-02  7.22656250e-02 -7.71484375e-02\n",
      "  1.66015625e-01 -1.31835938e-01  1.22070312e-01  1.18164062e-01\n",
      " -1.34277344e-02  1.85546875e-01  3.27148438e-02  1.61132812e-01\n",
      " -1.20605469e-01 -1.10839844e-01 -9.81445312e-02 -6.13403320e-03\n",
      " -1.14746094e-01 -1.21582031e-01 -2.32421875e-01 -1.10351562e-01\n",
      "  4.54101562e-02 -9.81445312e-02  5.24902344e-02 -2.33459473e-03]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):  # Check first 5 examples\n",
    "    print(\"X_train Sample:\", x_train[i])\n",
    "    print(\"Y_train Sample:\", y_train[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc96f7ed-14b7-4c2e-a594-3a71db4635b0",
   "metadata": {},
   "source": [
    "#### Bidirectional LSTM model to predict the masked words in sentences\n",
    "A bidirectional LSTM model has been utilized to analyze the context of different words masked in the training data with reference to the different parts of speech and make prediction as close as possible to the actual masked word. ***The prediction is a raw 300D embedding made by using two BiLSTM layers with units 256 and 128 respectively(fine tuned as per the model's performance) and based on a combined loss evaluation metric. The combined loss is the mean squared error loss(mse) and cosine similarity loss( custom functioned designed to return 1-cosine similarity, closer the loss to 0, higher the cosine similarity). This is done, firstly to reduce absolute distance from the actual word embedding and also considering the fact that synonymous words may differ in magintude as vectors but are oriented in the same direction as meaured by cosine similarity, thus predicting actual close embedding but also taking into account the synonymous context.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71e6bf38-4eac-42c0-9b7e-633ea839a34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 252ms/step - cosine_loss: 0.6169 - loss: 0.1977 - mse: 0.0180 - val_cosine_loss: 0.3047 - val_loss: 0.0948 - val_mse: 0.0048\n",
      "Epoch 2/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 254ms/step - cosine_loss: 0.5295 - loss: 0.1696 - mse: 0.0154 - val_cosine_loss: 0.2795 - val_loss: 0.0871 - val_mse: 0.0046\n",
      "Epoch 3/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 235ms/step - cosine_loss: 0.5065 - loss: 0.1626 - mse: 0.0152 - val_cosine_loss: 0.2337 - val_loss: 0.0733 - val_mse: 0.0046\n",
      "Epoch 4/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 240ms/step - cosine_loss: 0.4885 - loss: 0.1569 - mse: 0.0148 - val_cosine_loss: 0.2452 - val_loss: 0.0767 - val_mse: 0.0045\n",
      "Epoch 5/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 223ms/step - cosine_loss: 0.4765 - loss: 0.1533 - mse: 0.0148 - val_cosine_loss: 0.2486 - val_loss: 0.0777 - val_mse: 0.0045\n",
      "Epoch 6/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 186ms/step - cosine_loss: 0.4669 - loss: 0.1503 - mse: 0.0146 - val_cosine_loss: 0.2472 - val_loss: 0.0772 - val_mse: 0.0044\n",
      "Epoch 7/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 167ms/step - cosine_loss: 0.4563 - loss: 0.1470 - mse: 0.0144 - val_cosine_loss: 0.2509 - val_loss: 0.0784 - val_mse: 0.0044\n",
      "Epoch 8/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 174ms/step - cosine_loss: 0.4469 - loss: 0.1441 - mse: 0.0143 - val_cosine_loss: 0.2348 - val_loss: 0.0736 - val_mse: 0.0045\n",
      "Epoch 9/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 189ms/step - cosine_loss: 0.4342 - loss: 0.1402 - mse: 0.0142 - val_cosine_loss: 0.2585 - val_loss: 0.0807 - val_mse: 0.0044\n",
      "Epoch 10/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 170ms/step - cosine_loss: 0.4269 - loss: 0.1379 - mse: 0.0141 - val_cosine_loss: 0.2373 - val_loss: 0.0744 - val_mse: 0.0046\n",
      "Epoch 11/11\n",
      "\u001b[1m683/683\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 180ms/step - cosine_loss: 0.4190 - loss: 0.1355 - mse: 0.0140 - val_cosine_loss: 0.2484 - val_loss: 0.0777 - val_mse: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">1,140,736</span> \n",
       "\n",
       " dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> \n",
       "\n",
       " dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">77,100</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m512\u001b[0m)             \u001b[38;5;34m1,140,736\u001b[0m \n",
       "\n",
       " dropout_4 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m656,384\u001b[0m \n",
       "\n",
       " dropout_5 (\u001b[38;5;33mDropout\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " dense_2 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                    \u001b[38;5;34m77,100\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,622,662</span> (21.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,622,662\u001b[0m (21.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,874,220</span> (7.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,874,220\u001b[0m (7.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,748,442</span> (14.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,748,442\u001b[0m (14.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Define Cosine Similarity Loss\n",
    "def cosine_loss(y_true, y_pred):\n",
    "    y_true = tf.nn.l2_normalize(y_true, axis=-1)  # Normalize true embeddings\n",
    "    y_pred = tf.nn.l2_normalize(y_pred, axis=-1)  # Normalize predicted embeddings\n",
    "    return 1 - tf.reduce_mean(tf.reduce_sum(y_true * y_pred, axis=-1))  # 1 - cosine similarity\n",
    "\n",
    "# Combined Loss Function (MSE + Cosine Loss)\n",
    "def combined_loss(y_true, y_pred, alpha=0.7, beta=0.3):\n",
    "    mse_loss = tf.keras.losses.MSE(y_true, y_pred)\n",
    "    cos_loss = cosine_loss(y_true, y_pred)\n",
    "    return alpha * mse_loss + beta * cos_loss  # Weighted combination(fine tuned as per the model's performance)\n",
    "\n",
    "\n",
    "# Define BiLSTM Model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(256, return_sequences=True, input_shape=(40, 300))),  # BiLSTM layer\n",
    "    Dropout(0.3), #Dropout to prevent overfitting\n",
    "    Bidirectional(LSTM(128, return_sequences=False)),  # Final BiLSTM (outputs last hidden state)\n",
    "    Dropout(0.3), #Dropout to prevent overfitting\n",
    "    Dense(300, activation=\"linear\")  # Predicts raw word embeddings\n",
    "])\n",
    "\n",
    "model.compile(loss=lambda y_true, y_pred: combined_loss(y_true, y_pred, alpha=0.7, beta=0.3),\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=[\"mse\", cosine_loss])\n",
    "\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=64,\n",
    "    epochs=11,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "#Printing a summary of all params(trainable, non-trainable) along with the description of each layer in the biLSTM\n",
    "model.save(\"my_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ca9ff-92a5-4311-ad75-0adf86a2dde6",
   "metadata": {},
   "source": [
    "#### Text pre-processing of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7c6d2bc-5566-4211-9c50-0e9bb7262af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDS</th>\n",
       "      <th>MASKED SENTENCES</th>\n",
       "      <th>TOKENS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>the sweat stood upon it in &lt;MASKED&gt;</td>\n",
       "      <td>[the, sweat, stood, upon, it, in, &lt;MASKED&gt;, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>the city was named for judge &lt;MASKED&gt; r mckee</td>\n",
       "      <td>[the, city, was, named, for, judge, &lt;MASKED&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>a &lt;MASKED&gt; of girls are cheering</td>\n",
       "      <td>[a, &lt;MASKED&gt;, of, girls, are, cheering, ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>tom resigned as he wasnt &lt;MASKED&gt; valued at work</td>\n",
       "      <td>[tom, resigned, as, he, wasnt, &lt;MASKED&gt;, value...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>in the disastrous days that followed maurice w...</td>\n",
       "      <td>[in, the, disastrous, days, that, followed, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>many legume seeds have been proven to contain ...</td>\n",
       "      <td>[many, legume, seeds, have, been, proven, to, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>the name was for mr square the philosopher &lt;MA...</td>\n",
       "      <td>[the, name, was, for, mr, square, the, philoso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>fonaris gardenline the pat &lt;MASKED&gt; show and t...</td>\n",
       "      <td>[fonaris, gardenline, the, pat, &lt;MASKED&gt;, show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>a group of six people men and women hold up a ...</td>\n",
       "      <td>[a, group, of, six, people, men, and, women, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>a &lt;MASKED&gt; of a racer from gokart street race</td>\n",
       "      <td>[a, &lt;MASKED&gt;, of, a, racer, from, gokart, stre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDS                                   MASKED SENTENCES  \\\n",
       "0    1               the sweat stood upon it in <MASKED>    \n",
       "1    2     the city was named for judge <MASKED> r mckee    \n",
       "2    3                  a <MASKED> of girls are cheering    \n",
       "3    4  tom resigned as he wasnt <MASKED> valued at work    \n",
       "4    5  in the disastrous days that followed maurice w...   \n",
       "5    6  many legume seeds have been proven to contain ...   \n",
       "6    7  the name was for mr square the philosopher <MA...   \n",
       "7    8  fonaris gardenline the pat <MASKED> show and t...   \n",
       "8    9  a group of six people men and women hold up a ...   \n",
       "9   10     a <MASKED> of a racer from gokart street race    \n",
       "\n",
       "                                              TOKENS  \n",
       "0      [the, sweat, stood, upon, it, in, <MASKED>, ]  \n",
       "1  [the, city, was, named, for, judge, <MASKED>, ...  \n",
       "2          [a, <MASKED>, of, girls, are, cheering, ]  \n",
       "3  [tom, resigned, as, he, wasnt, <MASKED>, value...  \n",
       "4  [in, the, disastrous, days, that, followed, ma...  \n",
       "5  [many, legume, seeds, have, been, proven, to, ...  \n",
       "6  [the, name, was, for, mr, square, the, philoso...  \n",
       "7  [fonaris, gardenline, the, pat, <MASKED>, show...  \n",
       "8  [a, group, of, six, people, men, and, women, h...  \n",
       "9  [a, <MASKED>, of, a, racer, from, gokart, stre...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "test_df=pd.read_csv(\"test_set_f.csv\") #reading test dataset into pandas dataframe\n",
    "\n",
    "import re\n",
    "\n",
    "def custom_preprocess(text):\n",
    "    words = text.split() #tokenizing sentences to words(since sentences in the test data are mainly plain sentences with space separated words)\n",
    "\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if word == \"<MASKED>\":  \n",
    "            processed_words.append(word)  # keeping <\"MASKED\"> token as it is\n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)  # Removing punctuation\n",
    "            processed_words.append(word)\n",
    "\n",
    "    return \" \".join(processed_words)\n",
    "\n",
    "test_df[\"MASKED SENTENCES\"] = test_df[\"MASKED SENTENCES\"].apply(custom_preprocess) \n",
    "\n",
    "def tokenize_test(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "test_df['TOKENS']=test_df['MASKED SENTENCES'].apply(tokenize_test) #creating a new Pandas series containing all tokens corresponding to every sentence\n",
    "\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8890cb3-0258-4eb0-bedf-04ab85bd4372",
   "metadata": {},
   "source": [
    "#### Generating x_test:\n",
    "After pre-processing and tokenizing of test data, the sentence words vectorized to their corresponding embeddings along with the \"MAKSED\" token converted to a specific manual embedding as defined for the training data. The x_test is a numpy array in the expected format for the model to predict raw embeddings for the missing(masked) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ddbc3d6-9931-4dd0-a268-70eafed2200b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (10000, 40, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Constants\n",
    "MAX_LEN = 40\n",
    "EMBEDDING_DIM = 300  \n",
    "\n",
    "# Load Google News Word2Vec Model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# Placeholder for masked token embedding\n",
    "MASKED_EMBEDDING = np.full((EMBEDDING_DIM,), 9.99)  # Random uniform embedding for masked words\n",
    "\n",
    "def words_to_embeddings(tokenized_sentences, word2vec_model):\n",
    "    x_test = []\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        embedded_sentence = []\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word == \"<MASKED>\":\n",
    "                embedded_sentence.append(MASKED_EMBEDDING)  # Use MASKED token embedding\n",
    "            elif word in word2vec_model:\n",
    "                embedded_sentence.append(word2vec_model[word])  # Get word embedding\n",
    "            else:\n",
    "                embedded_sentence.append(np.zeros(EMBEDDING_DIM))  # Handle unknown words\n",
    "        \n",
    "        # Padding/truncation to MAX_LEN\n",
    "        if len(embedded_sentence) < MAX_LEN:\n",
    "            embedded_sentence += [np.zeros(EMBEDDING_DIM)] * (MAX_LEN - len(embedded_sentence))\n",
    "        else:\n",
    "            embedded_sentence = embedded_sentence[:MAX_LEN]\n",
    "        \n",
    "        x_test.append(embedded_sentence)\n",
    "    \n",
    "    return np.array(x_test)\n",
    "tokenized_test_sentences = test_df[\"TOKENS\"].tolist()  \n",
    "\n",
    "# Convert tokenized test sentences into embeddings\n",
    "x_test = words_to_embeddings(tokenized_test_sentences, word2vec_model)\n",
    "\n",
    "print(f\"x_test shape: {x_test.shape}\")  # (num_samples, MAX_LEN, EMBEDDING_DIM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edde47a4-7051-4f62-bab8-a57d70979965",
   "metadata": {},
   "source": [
    "#### Final word prediction and storage into submission.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3f2f5-ba5e-4232-825d-e11b0730c4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the Word2Vec Model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "def closest_valid_word(predicted_vector, word2vec_model):\n",
    "    \"\"\"\n",
    "    Finds the closest valid word (only lowercase alphabets) in Word2Vec for the given embedding.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get top 12 most similar words\n",
    "        top_matches = word2vec_model.most_similar([predicted_vector], topn=12)\n",
    "\n",
    "        # Regex pattern to allow only lowercase words\n",
    "        alpha_pattern = re.compile(r\"^[a-z]+$\")  \n",
    "\n",
    "        # Filter valid lowercase words **without converting case**\n",
    "        for word, _ in top_matches:\n",
    "            if alpha_pattern.match(word):  \n",
    "                return word  # Return the first valid lowercase word\n",
    "        \n",
    "        return \"the\"  # If no valid word is found, return \"the\" as the fallback word\n",
    "\n",
    "    except KeyError:\n",
    "        return \"<UNK>\"\n",
    "\n",
    "# Predict embeddings from your trained model\n",
    "predicted_embeddings = model.predict(x_test)  # Shape: (num_samples, 300)\n",
    "\n",
    "# Convert embeddings to words\n",
    "predicted_words = [closest_valid_word(emb, word2vec_model) for emb in predicted_embeddings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f72acc8-0b50-475f-bfde-27d661f643ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blue', 'for', 'group', 'as', 'the', 'as', 'played', 'on', 'the', 'man', 'the', 'the', 'the', 'me', 'the', 'just', 'with', 'the', 'do', 'that', 'the', 'as', 'the', 'the', 'is', 'for', 'the', 'the', 'the', 'the', 'the', 'by', 'is', 'just', 'many', 'with', 'the', 'do', 'the', 'the', 'the', 'for', 'i', 'just', 'other', 'can', 'constructed', 'just', 'have', 'the', 'out', 'in', 'the', 'just', 'man', 'the', 'the', 'do', 'just', 'at', 'blue', 'has', 'just', 'the', 'north', 'the', 'out', 'going', 'very', 'the', 'are', 'first', 'the', 'in', 'on', 'as', 'many', 'sitting', 'the', 'the', 'by', 'on', 'the', 'the', 'that', 'the', 'out', 'the', 'man', 'the', 'people', 'are', 'the', 'in', 'the', 'the', 'the', 'also', 'the', 'it', 'the', 'the', 'the', 'the', 'just', 'is', 'is', 'just', 'the', 'can', 'down', 'is', 'the', 'out', 'the', 'have', 'there', 'for', 'the', 'the', 'the', 'the', 'sitting', 'the', 'very', 'white', 'the', 'the', 'the', 'on', 'just', 'the', 'give', 'the', 'in', 'know', 'just', 'the', 'the', 'in', 'the', 'the', 'the', 'the', 'very', 'at', 'also', 'the', 'man', 'the', 'also', 'the', 'the', 'also', 'the', 'the', 'have', 'the', 'the', 'for', 'the', 'came', 'the', 'the', 'the', 'the', 'the', 'never', 'the', 'for', 'the', 'also', 'with', 'that', 'the', 'just', 'the', 'man', 've', 'the', 'is', 'for', 'the', 'the', 'just', 'the', 'came', 'the', 'not', 'in', 'the', 'the', 'man', 'the', 'the', 'just', 'the', 'the', 'the', 'at', 'the', 'the', 'the', 'the', 'some', 'the', 'people', 'many', 'just', 'can', 'the', 'the', 'on', 'the', 'white', 'the', 'for', 'went', 'had', 'give', 'the', 'the', 'on', 'the', 'just', 'the', 'out', 'just', 'the', 'just', 'are', 'is', 'named', 'the', 'the', 'are', 'blue', 'for', 'the', 'the', 'the', 'the', 'out', 'the', 'man', 'the', 'the', 'are', 'the', 'at']\n"
     ]
    }
   ],
   "source": [
    "print(predicted_words[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "198384a1-239b-4146-83ab-48781f6651dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sweat stood upon it in <MASKED> \n",
      "Predicted word: \n",
      "blue\n",
      "the city was named for judge <MASKED> r mckee \n",
      "Predicted word: \n",
      "for\n",
      "a <MASKED> of girls are cheering \n",
      "Predicted word: \n",
      "group\n",
      "tom resigned as he wasnt <MASKED> valued at work \n",
      "Predicted word: \n",
      "as\n",
      "in the disastrous days that followed maurice was subject to fredericks <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "many legume seeds have been proven to contain high lectin <MASKED> termed hemagglutinating activity \n",
      "Predicted word: \n",
      "as\n",
      "the name was for mr square the philosopher <MASKED> in henry fieldings tom jones \n",
      "Predicted word: \n",
      "played\n",
      "fonaris gardenline the pat <MASKED> show and the handyman hotline with larry egan \n",
      "Predicted word: \n",
      "on\n",
      "a group of six people men and women hold up a pole in the <MASKED> of a forest while another woman monitors this action \n",
      "Predicted word: \n",
      "the\n",
      "a <MASKED> of a racer from gokart street race \n",
      "Predicted word: \n",
      "man\n",
      "drongen is the birthplace of belgian professional footballer kevin de <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "he became a free agent after <MASKED> season ended \n",
      "Predicted word: \n",
      "the\n",
      "whitwell <MASKED> to be twinned with paris france \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> children smile underwater \n",
      "Predicted word: \n",
      "me\n",
      "a virtual <MASKED> can allow for lowcost expansion with no longterm commitments \n",
      "Predicted word: \n",
      "the\n",
      "his greed and ambition know no <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "a man wearing an orange shirt <MASKED> a baseball glove has just thrown a ball \n",
      "Predicted word: \n",
      "with\n",
      "early <MASKED> include fever and flulike symptoms \n",
      "Predicted word: \n",
      "the\n",
      "is a language l expressive enough to <MASKED> a single finite structure s \n",
      "Predicted word: \n",
      "do\n",
      "in a speech to police he stated <MASKED> you cant do the cossacks can \n",
      "Predicted word: \n",
      "that\n",
      "besselsleigh has a public house <MASKED> greyhound \n",
      "Predicted word: \n",
      "the\n",
      "bach structured <MASKED> cantata in six movements \n",
      "Predicted word: \n",
      "as\n",
      "<MASKED> level has two tracks and an island platform \n",
      "Predicted word: \n",
      "the\n",
      "he submitted <MASKED> plan to the city council prior to leaving office \n",
      "Predicted word: \n",
      "the\n",
      "he <MASKED> smoking a long clay pipe and studying the water with spectacled eyes \n",
      "Predicted word: \n",
      "is\n",
      "these equations <MASKED> the configuration of the chain in terms of its joint parameters \n",
      "Predicted word: \n",
      "for\n",
      "a man on a bike in <MASKED> city area \n",
      "Predicted word: \n",
      "the\n",
      "are you strong <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "two notable exceptions are <MASKED> to the death of members of his family \n",
      "Predicted word: \n",
      "the\n",
      "the <MASKED> sealers office \n",
      "Predicted word: \n",
      "the\n",
      "the <MASKED> in pregnancy of one ingredient centella asiatica has been questioned \n",
      "Predicted word: \n",
      "the\n",
      "the content of the series was softcore adult films <MASKED> european \n",
      "Predicted word: \n",
      "by\n",
      "the universal baseball <MASKED> incorporated j henry waugh prop \n",
      "Predicted word: \n",
      "is\n",
      "thats a pack of lies youre <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "the company owns <MASKED> restaurants and hotels \n",
      "Predicted word: \n",
      "many\n",
      "coomes has done much recording work <MASKED> other bands mostly on keyboards and bass \n",
      "Predicted word: \n",
      "with\n",
      "the history entitled amiable warriors is being <MASKED> in three volumes \n",
      "Predicted word: \n",
      "the\n",
      "i will <MASKED> up this case \n",
      "Predicted word: \n",
      "do\n",
      "i recommend investing in a <MASKED> microphone \n",
      "Predicted word: \n",
      "the\n",
      "pronouns in these languages tend to be <MASKED> genderneutral \n",
      "Predicted word: \n",
      "the\n",
      "she subsequently took his surname <MASKED> is now named melissa beck \n",
      "Predicted word: \n",
      "the\n",
      "examples of thermosetting adhesives are epoxy polyurethane <MASKED> and acrylic polymers \n",
      "Predicted word: \n",
      "for\n",
      "<MASKED> won their first asian cup of all time \n",
      "Predicted word: \n",
      "i\n",
      "<MASKED> emphasised the point by waving her hands \n",
      "Predicted word: \n",
      "just\n",
      "dishes may range from simple to elaborate <MASKED> dishes incorporate maize and moles \n",
      "Predicted word: \n",
      "other\n",
      "it <MASKED> not be so noticeable there \n",
      "Predicted word: \n",
      "can\n",
      "baldurs gate is <MASKED> by four grand dukes the council of four \n",
      "Predicted word: \n",
      "constructed\n",
      "a man is <MASKED> watching a small tv \n",
      "Predicted word: \n",
      "just\n",
      "and look how many things <MASKED> wind already knew how to do \n",
      "Predicted word: \n",
      "have\n",
      "liverpool was officially opened by frank <MASKED> and the oliveri family alongside ald \n",
      "Predicted word: \n",
      "the\n",
      "a startled exclamation burst <MASKED> ricardo \n",
      "Predicted word: \n",
      "out\n",
      "a downtown city <MASKED> a person is leaning up against a building while on his cellphone \n",
      "Predicted word: \n",
      "in\n",
      "<MASKED> only pub now remaining in lovell park is the leeds rifleman \n",
      "Predicted word: \n",
      "the\n",
      "no compliments pray without ceremony without the organ without <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "an electric <MASKED> was waiting \n",
      "Predicted word: \n",
      "man\n",
      "a connecticut native winfield met her husband at a <MASKED> concert \n",
      "Predicted word: \n",
      "the\n",
      "i have a <MASKED> disc \n",
      "Predicted word: \n",
      "the\n",
      " i mean <MASKED> i say  the mock turtle replied in an offended tone \n",
      "Predicted word: \n",
      "do\n",
      "i was right <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "helene set the flask down <MASKED> the table \n",
      "Predicted word: \n",
      "at\n",
      "a little girl wearing a cardboard <MASKED> hat is finishing off some onion rings at a restaurant \n",
      "Predicted word: \n",
      "blue\n",
      "the district currently <MASKED> two villages \n",
      "Predicted word: \n",
      "has\n",
      "their song down in my heart was featured in the series finale of <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "show me <MASKED> schedule in the nearest movie theatre for invasion usa \n",
      "Predicted word: \n",
      "the\n",
      "it spans the bitterroot mountain range on the border between idaho <MASKED> montana \n",
      "Predicted word: \n",
      "north\n",
      "this will be made clearer by plots of the <MASKED> density functions \n",
      "Predicted word: \n",
      "the\n",
      "a girl showing an art <MASKED> \n",
      "Predicted word: \n",
      "out\n",
      "disputationes were <MASKED> to resolve controversial quaestiones \n",
      "Predicted word: \n",
      "going\n",
      "four republican temples were part of the <MASKED> complex \n",
      "Predicted word: \n",
      "very\n",
      "the governor appoints ministers advisors and the <MASKED> directors to the counties \n",
      "Predicted word: \n",
      "the\n",
      "so hughie and i <MASKED> the managing ourselves \n",
      "Predicted word: \n",
      "are\n",
      "richards good qualities were his <MASKED> and bravery \n",
      "Predicted word: \n",
      "first\n",
      "<MASKED> bell from the previous courthouse clock tower sits on the courthouse grounds \n",
      "Predicted word: \n",
      "the\n",
      "now <MASKED> a minute \n",
      "Predicted word: \n",
      "in\n",
      "its citizens <MASKED> the highest average household income in oklahoma \n",
      "Predicted word: \n",
      "on\n",
      "fair market value is not explicitly defined in the income <MASKED> act \n",
      "Predicted word: \n",
      "as\n",
      "in addition any well <MASKED> interpretations or commentaries that exist are a necessity \n",
      "Predicted word: \n",
      "many\n",
      "a group of men sit around a table that has <MASKED> bottles on it \n",
      "Predicted word: \n",
      "sitting\n",
      "he earned his first allstar <MASKED> selection voted in on the players ballot \n",
      "Predicted word: \n",
      "the\n",
      "by electrically stimulating the vagus nerve loewi made the first heart beat <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "lyrics and screenplay were written by howard <MASKED> \n",
      "Predicted word: \n",
      "by\n",
      "people walk <MASKED> a path in a village in india \n",
      "Predicted word: \n",
      "on\n",
      "<MASKED> is the most important source of carbohydrate in a majority of countries \n",
      "Predicted word: \n",
      "the\n",
      "it is contraindicated in patients with advanced malignant <MASKED> tumors \n",
      "Predicted word: \n",
      "the\n",
      "at the lyric he directed productions of classic plays which he translated or <MASKED> \n",
      "Predicted word: \n",
      "that\n",
      "he studied law at the <MASKED> and then entered the government service \n",
      "Predicted word: \n",
      "the\n",
      "play music through groove <MASKED> \n",
      "Predicted word: \n",
      "out\n",
      "paulding <MASKED> chiefly selfeducated \n",
      "Predicted word: \n",
      "the\n",
      "a <MASKED> climbing stone stairs surrounded by plants \n",
      "Predicted word: \n",
      "man\n",
      "in time pennsylvania avenue developed into the capital citys <MASKED> grand avenue \n",
      "Predicted word: \n",
      "the\n",
      "two <MASKED> on hind legs and two seated dogs are looking at something in the air \n",
      "Predicted word: \n",
      "people\n",
      "<MASKED> rich and diverse archaeological findings attest to strong international links and trade relations \n",
      "Predicted word: \n",
      "are\n",
      "when you play cards <MASKED> first time you are almost sure to win \n",
      "Predicted word: \n",
      "the\n",
      "for example prang brahmadat was built of laterite <MASKED> that form a square \n",
      "Predicted word: \n",
      "in\n",
      "on <MASKED> surface the two main characters have little in common \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> new zealand kordia network is based primarily on digital microwave technology \n",
      "Predicted word: \n",
      "the\n",
      "he also <MASKED> ties through marriage to a large group of lycian nobles \n",
      "Predicted word: \n",
      "the\n",
      "wants are <MASKED> distinguished from needs \n",
      "Predicted word: \n",
      "also\n",
      "he was married four times and <MASKED> eighteen children \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> says they were red \n",
      "Predicted word: \n",
      "it\n",
      "how many layers of <MASKED> are you on \n",
      "Predicted word: \n",
      "the\n",
      "he has been <MASKED> to transcend time by reincarnation and eternal youth \n",
      "Predicted word: \n",
      "the\n",
      "the coast guard ship arrived several days later due to its <MASKED> cruising speed \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> most residents on this side of humboldt park are upper middle class residents \n",
      "Predicted word: \n",
      "the\n",
      " that  s just what i complain of  said humpty <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "the line <MASKED> reopened by british rail and strathclyde passenger transport executive \n",
      "Predicted word: \n",
      "is\n",
      "asian street <MASKED> waiting to sell there merchandise \n",
      "Predicted word: \n",
      "is\n",
      "she was disappointed that readers did not enjoy kurtzmans <MASKED> look \n",
      "Predicted word: \n",
      "just\n",
      "it cant happen here is number one on the list of <MASKED> last words \n",
      "Predicted word: \n",
      "the\n",
      "i dreaded what he <MASKED> think \n",
      "Predicted word: \n",
      "can\n",
      "a person hanging <MASKED> of a cliff \n",
      "Predicted word: \n",
      "down\n",
      "the belt buckle <MASKED> obnoxiously big \n",
      "Predicted word: \n",
      "is\n",
      "a <MASKED> rider is in midair while on a dirt trail \n",
      "Predicted word: \n",
      "the\n",
      "two brown dogs running <MASKED> water \n",
      "Predicted word: \n",
      "out\n",
      "press eject to release the <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "the requirements of this assignment <MASKED> been defined below \n",
      "Predicted word: \n",
      "have\n",
      "yes yes <MASKED> are strong differences \n",
      "Predicted word: \n",
      "there\n",
      "that same year langley won the only two races of <MASKED> long career \n",
      "Predicted word: \n",
      "for\n",
      "the <MASKED> release is currently on hold \n",
      "Predicted word: \n",
      "the\n",
      "there is also a manual door on the offside into the drivers <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "he was the <MASKED> chief executive to be brought in from outside the organization \n",
      "Predicted word: \n",
      "the\n",
      "i used <MASKED> euro tunnel this time around \n",
      "Predicted word: \n",
      "the\n",
      "a young girl with a white shirt <MASKED> has rainbows on it eats a pastry \n",
      "Predicted word: \n",
      "sitting\n",
      "he nodded to her with a friendly <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "the spiritual anguish induced by <MASKED> tactless speech had become noticeably lessened \n",
      "Predicted word: \n",
      "very\n",
      "the firstbuilt <MASKED> limousine was donated to blue house \n",
      "Predicted word: \n",
      "white\n",
      "they were <MASKED> core members of the portland spy ring \n",
      "Predicted word: \n",
      "the\n",
      "a man and his <MASKED> at the lake \n",
      "Predicted word: \n",
      "the\n",
      "in rugby union rugby league <MASKED> association football they are known as touchlines \n",
      "Predicted word: \n",
      "the\n",
      "someone climbing a rock upside <MASKED> \n",
      "Predicted word: \n",
      "on\n",
      "breakfast is usually <MASKED> than this she thought \n",
      "Predicted word: \n",
      "just\n",
      "a man in a green kilt is relaxing on <MASKED> sofa \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> to my playlist called rock scratch my back \n",
      "Predicted word: \n",
      "give\n",
      "it is <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "japan rounding up their involvement <MASKED> that anime series \n",
      "Predicted word: \n",
      "in\n",
      "<MASKED> me no more child \n",
      "Predicted word: \n",
      "know\n",
      "let me have it <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "a eurosceptic she is on the council of the rightwing <MASKED> way forward group \n",
      "Predicted word: \n",
      "the\n",
      "they were beaten by max <MASKED> and daniel nestor \n",
      "Predicted word: \n",
      "the\n",
      "the knicks success continued <MASKED> the next few years \n",
      "Predicted word: \n",
      "in\n",
      "in saturday showdown a revised <MASKED> of ram ray is played \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> anglosaxon language is commonly known as old english \n",
      "Predicted word: \n",
      "the\n",
      "a young boy plays in a shallow <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "the <MASKED> must allow for this every time these words are used \n",
      "Predicted word: \n",
      "the\n",
      "enfield described them as a <MASKED> more highbrow than the other characters \n",
      "Predicted word: \n",
      "very\n",
      "<MASKED> the time when he was left in the loftroom he vanished \n",
      "Predicted word: \n",
      "at\n",
      "i married marge <MASKED> written by jeff martin and directed by jeffrey lynch \n",
      "Predicted word: \n",
      "also\n",
      "in mario super <MASKED> he appears as a captain again \n",
      "Predicted word: \n",
      "the\n",
      "a <MASKED> plays a guitar while sitting on bleachers in front of a lake \n",
      "Predicted word: \n",
      "man\n",
      "he is an economist specializing in social <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "commensal pathway <MASKED> include dogs cats fowl and possibly pigs \n",
      "Predicted word: \n",
      "also\n",
      "the airstrip remains today and daily flights land from <MASKED> and gizo \n",
      "Predicted word: \n",
      "the\n",
      "ill have a flat white <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "she <MASKED> also appeared in films \n",
      "Predicted word: \n",
      "also\n",
      "there are even references to a more rock and <MASKED> sound \n",
      "Predicted word: \n",
      "the\n",
      "a band <MASKED> is performing in front of their many followers \n",
      "Predicted word: \n",
      "the\n",
      "the region is one of the worlds largest suppliers of <MASKED> and copper ores \n",
      "Predicted word: \n",
      "have\n",
      "a policeman is standing with a german shepherd <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "since the <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "the symbol of the currency was the abbreviation k or <MASKED> kr \n",
      "Predicted word: \n",
      "for\n",
      "he is often booed loudly by the audience when <MASKED> out on the set \n",
      "Predicted word: \n",
      "the\n",
      "and the queen <MASKED> to guess her thoughts for she cried  faster  \n",
      "Predicted word: \n",
      "came\n",
      "these and later extensions to the <MASKED> were numbered sequentially \n",
      "Predicted word: \n",
      "the\n",
      "there were larks linnets and goldfinches  i should think at <MASKED> twenty \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> host nation is canada \n",
      "Predicted word: \n",
      "the\n",
      "rapid residential and commercial development in the watershed is <MASKED> stormwater runoff \n",
      "Predicted word: \n",
      "the\n",
      "jim has a crush on judy <MASKED> judys boyfriend buzz is a popular jock \n",
      "Predicted word: \n",
      "the\n",
      "he thought of the woman <MASKED> had trusted in the desert \n",
      "Predicted word: \n",
      "never\n",
      "northern kankanaey is listed as a <MASKED> language \n",
      "Predicted word: \n",
      "the\n",
      "many people waiting in long lines to get <MASKED> tickets \n",
      "Predicted word: \n",
      "for\n",
      "somewhere you are holding the <MASKED> i love the boy said \n",
      "Predicted word: \n",
      "the\n",
      "the race <MASKED> won by british driver john surtees driving a honda \n",
      "Predicted word: \n",
      "also\n",
      "christmas in ireland has several local traditions some in no way connected <MASKED> christianity \n",
      "Predicted word: \n",
      "with\n",
      "besides <MASKED> there was a heap of bicycles \n",
      "Predicted word: \n",
      "that\n",
      "a <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "schenn sustained no apparent injury due to <MASKED> hit \n",
      "Predicted word: \n",
      "just\n",
      "moreover <MASKED> precise criteria for defining ethnic groups varies considerably \n",
      "Predicted word: \n",
      "the\n",
      "a male <MASKED> is instructing his class of young children \n",
      "Predicted word: \n",
      "man\n",
      "you could <MASKED> knocked me down with a feather engaged to him \n",
      "Predicted word: \n",
      "ve\n",
      "modern geographers and hydrographers however have claimed that ancient <MASKED> was an island \n",
      "Predicted word: \n",
      "the\n",
      "it is a symmetric logistic distribution <MASKED> often confused with the normal gaussian function \n",
      "Predicted word: \n",
      "is\n",
      "information was drawn from nourse family correspondence and joseph nourses account <MASKED> \n",
      "Predicted word: \n",
      "for\n",
      "here comes the <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "the construction works was awarded to <MASKED> sdn bhd \n",
      "Predicted word: \n",
      "the\n",
      "but i didn  t think <MASKED> it contained any living creature \n",
      "Predicted word: \n",
      "just\n",
      "it was built under the supervision of raffaele daniele the <MASKED> of the brotherhood \n",
      "Predicted word: \n",
      "the\n",
      "clarke <MASKED> the hastings team and he signed his first professional contract \n",
      "Predicted word: \n",
      "came\n",
      "he served in a variety of <MASKED> in the middleeast after the sixday war \n",
      "Predicted word: \n",
      "the\n",
      "there are no words to describe how difficult that day <MASKED> \n",
      "Predicted word: \n",
      "not\n",
      "<MASKED> the cambria county seat is to the northeast \n",
      "Predicted word: \n",
      "in\n",
      "the staircase ascends to the <MASKED> of the high rise building \n",
      "Predicted word: \n",
      "the\n",
      "she was the first woman to earn a phd in the united <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> with guitar holding a microphone in one hand and extending his other arm \n",
      "Predicted word: \n",
      "man\n",
      "<MASKED> petroleum gas was used for cooking heating and lighting \n",
      "Predicted word: \n",
      "the\n",
      "the indians of towamencin <MASKED> are of the delaware nation \n",
      "Predicted word: \n",
      "the\n",
      "it was a cute little puppy with huge <MASKED> eyes \n",
      "Predicted word: \n",
      "just\n",
      "<MASKED> main lighted runway is long \n",
      "Predicted word: \n",
      "the\n",
      "according to <MASKED> legend the new jersey devil has been spotted by balls ferry \n",
      "Predicted word: \n",
      "the\n",
      "he didnt run off or <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "they were constructed <MASKED> dual threelane carriageways \n",
      "Predicted word: \n",
      "at\n",
      "planned extension from spring creek to <MASKED> county line \n",
      "Predicted word: \n",
      "the\n",
      "i want my <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> of the swiss delegation to the interparliamentary union \n",
      "Predicted word: \n",
      "the\n",
      "it can also be found in <MASKED> canary islands \n",
      "Predicted word: \n",
      "the\n",
      "<MASKED> extrathin extralong tapes are rare and expensive \n",
      "Predicted word: \n",
      "some\n",
      "barber and dan sikes were declared <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "three <MASKED> in matching pink dresses standing on the beach \n",
      "Predicted word: \n",
      "people\n",
      "<MASKED> and wood had offices in the world trade center \n",
      "Predicted word: \n",
      "many\n",
      "if i knew it i would <MASKED> told you \n",
      "Predicted word: \n",
      "just\n",
      "to <MASKED> we need to work diligently \n",
      "Predicted word: \n",
      "can\n",
      "<MASKED> spring has been known as the lewis spring since that time \n",
      "Predicted word: \n",
      "the\n",
      "jashpur is now a <MASKED> district \n",
      "Predicted word: \n",
      "the\n",
      "the site where these settlers situated <MASKED> the mountains used to be called conventa \n",
      "Predicted word: \n",
      "on\n",
      "two men in shorts wrestle in <MASKED> short grass \n",
      "Predicted word: \n",
      "the\n",
      "man reading a navigator on a <MASKED> \n",
      "Predicted word: \n",
      "white\n",
      "john lennon visited many japanese shinto shrines and <MASKED> of them is yasukuni shrine \n",
      "Predicted word: \n",
      "the\n",
      "oh come all ye <MASKED> \n",
      "Predicted word: \n",
      "for\n",
      " who <MASKED> at it again  she ventured to ask \n",
      "Predicted word: \n",
      "went\n",
      "they <MASKED> a boat and went down the river \n",
      "Predicted word: \n",
      "had\n",
      "we shall <MASKED> the recess \n",
      "Predicted word: \n",
      "give\n",
      "jacksonville state university also has an <MASKED> building named bibb graves hall \n",
      "Predicted word: \n",
      "the\n",
      "i did a <MASKED> on silicon chip production and built a proximity sensor for activating garage lights when a car entered \n",
      "Predicted word: \n",
      "the\n",
      "a couple sit <MASKED> of an establishment \n",
      "Predicted word: \n",
      "on\n",
      "when he arrives he has to <MASKED> and the jail guard treats him rudely \n",
      "Predicted word: \n",
      "the\n",
      "something is <MASKED> flying around \n",
      "Predicted word: \n",
      "just\n",
      "we must take care of our dear <MASKED> miss summerson \n",
      "Predicted word: \n",
      "the\n",
      "when we find <MASKED> they are dapper \n",
      "Predicted word: \n",
      "out\n",
      "i shall owe you you dont know how <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "the noun is often an instrumental <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "please see the graph <MASKED> \n",
      "Predicted word: \n",
      "just\n",
      "all of the organs <MASKED> exposed in our exhibition hall \n",
      "Predicted word: \n",
      "are\n",
      "a group of children <MASKED> a snack listening to a lecture \n",
      "Predicted word: \n",
      "is\n",
      "the origins of the arraus can be <MASKED> in barcelona and further in france \n",
      "Predicted word: \n",
      "named\n",
      "please use your full <MASKED> including postcode \n",
      "Predicted word: \n",
      "the\n",
      "she was not <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "two other novels <MASKED> available in french translation \n",
      "Predicted word: \n",
      "are\n",
      "the fringe is yellow the cord and tassels are medium blue and <MASKED> \n",
      "Predicted word: \n",
      "blue\n",
      "i  ve not had a bite to eat <MASKED> two days \n",
      "Predicted word: \n",
      "for\n",
      "plans <MASKED> a greenways project are moving forward \n",
      "Predicted word: \n",
      "the\n",
      "the <MASKED> is porous \n",
      "Predicted word: \n",
      "the\n",
      "camphor hall is a hall at dillard <MASKED> new orleans \n",
      "Predicted word: \n",
      "the\n",
      "one version featured original <MASKED> member bivins plus hardy hemphill pazant and williams \n",
      "Predicted word: \n",
      "the\n",
      "butters then happily leaves getting ready to wreak havoc once <MASKED> \n",
      "Predicted word: \n",
      "out\n",
      "<MASKED> is an incubator for innovation \n",
      "Predicted word: \n",
      "the\n",
      "a <MASKED> with glasses on cooking \n",
      "Predicted word: \n",
      "man\n",
      "after retiring <MASKED> competition everson began acting \n",
      "Predicted word: \n",
      "the\n",
      "the salem river flows along the townships eastern and southern <MASKED> \n",
      "Predicted word: \n",
      "the\n",
      "students <MASKED> zoned to krop high school \n",
      "Predicted word: \n",
      "are\n",
      "metroplex subsequently returned all of the monuments to their <MASKED> residences \n",
      "Predicted word: \n",
      "the\n",
      "anthony was <MASKED> four times \n",
      "Predicted word: \n",
      "at\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for i in range(250):\n",
    "    print(test_df['MASKED SENTENCES'][i])\n",
    "    print(\"Predicted word: \")\n",
    "    print(predicted_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "638138f3-3323-4afa-988b-4e08b070208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'predicted_words.csv' has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with IDS and PREDICTED WORDS\n",
    "df = pd.DataFrame({\n",
    "    \"IDS\": range(len(predicted_words)),  # Index column\n",
    "    \"PREDICTED WORDS\": predicted_words   # Predicted words column\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"predicted_words.csv\", index=False)\n",
    "\n",
    "print(\"CSV file 'predicted_words.csv' has been saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
